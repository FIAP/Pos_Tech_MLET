{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438fe21c-597e-48ef-8c7f-6b7bb9d03c43",
   "metadata": {},
   "source": [
    "# Classificação de Texto e Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9665361-2cfc-4ef6-bb99-243d87fe0f21",
   "metadata": {},
   "source": [
    "Olá, turma!\n",
    "\n",
    "Nesta aula, estudaremos o problema de classificação de texto. Iremos pegar uma base de texto, usar as técnicas de pré-processamento vistas na última aula e preparar os dados para inseri-los num algoritmo de machine learning. Por fim, veremos como podemos estender o conceito de classificação de texto para análise de sentimentos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb129a07-9f23-45f4-b467-d056dbd0f6b9",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd49df5-6e26-44ba-beda-2707e6cf3e28",
   "metadata": {},
   "source": [
    "Para começarmos a entender o problema de classificação de texto, sem antes dar uma definição formal, vamos considerar alguns exemplos. Considere a imagem abaixo. Você a classificaria como um spam ou como uma mensagem legítima?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49215f1-72f5-4eea-ae2f-c9a64d34e795",
   "metadata": {},
   "source": [
    "<img src=\"img/spam.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa82e7-5287-4eb4-91de-cbff39499cec",
   "metadata": {},
   "source": [
    "Olhando o texto escrito, é possível dizer que existe alguma característica que permita a nós dizer se essa mensagem é fraudulenta ou não? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56eedd-7097-499d-bf75-dde0d6854315",
   "metadata": {},
   "source": [
    "Vamos considerar mais um exemplo, retira de um artigo por Argamon et al, em 2003. Considere essas duas frases:\n",
    "\n",
    "1.\tBy 1925 present-day Vietnam was divided into three parts under French colonial rule. The southern region embracing Saigon and the Mekong delta was the colony of Cochin-China; the central area with its imperial capital at Hue was the protectorate of Annam…\n",
    "\n",
    "2.\tClara never failed to be astonished by the extraordinary felicity of her own name. She found it hard to trust herself to the mercy of fate, which had managed over the years to convert her greatest shame into one of her greatest assets…\n",
    "\n",
    "Analisando essas duas frases, você consegue identificar o gênero de quem as escreveu? Apesar de não ser uma tarefa trivial, é possível, com dados bons e em quantidade suficiente, treinar um classificador para identificar o gênero do autor de um texto.\n",
    "\n",
    "\n",
    "Mais um exemplo. Considere as seguintes críticas escritas de uma maneira bem resumida:\n",
    "* Incrivelmente desapontador\n",
    "* Cheio de personagens mirabolantes e uma sátira ricamente aplicada\n",
    "* O melhor filme de comédia já feito\n",
    "* Foi patético. A pior parte foi a cena dentro do saguão.\n",
    "\n",
    "Ok, aqui é um pouco mais fácil, já que é algo que vemos quase que diariamente. Mas temos algumas questões importantes: Como você conseguiu diferenciar uma crítica negativa de uma positiva? Que critérios você usou para estabelecer essa diferença? Foram as palavras, o contexto, ambos? \n",
    "\n",
    "Apesar de descobrirmos o tipo de crítica logo após a ler a frase, precisamos pensar em como nosso cérebro processou as informações para chegar nessa conclusão, pois isso é o que devemos fazer, mas passo a passo, para que o modelo também consiga distinguir uma crítica positiva de uma negativa. \n",
    "\n",
    "Por esses exemplos, podemos ver que existe uma grande quantidade de aplicações que envolvem classificação de textos. Aqui vão mais algumas:\n",
    "\n",
    "* Atribuir categorias, tópicos ou gêneros a assuntos\n",
    "* Classificação de mensagens textuais\n",
    "* Identificação autoral\n",
    "* Identificação de língua escrita\n",
    "* Classificação de sentimento\n",
    "* Chatbots\n",
    "\n",
    "Embora o propósito e a aplicação da classificação de texto possam variar de domínio para domínio, o problema abstrato subjacente permanece o mesmo. Essa invariância do problema central e suas aplicações em uma infinidade de domínios torna a classificação de texto de longe a tarefa de NLP mais utilizada na indústria e a mais pesquisada na academia.\n",
    "\n",
    "Em machine learning, a classificação é o problema de categorizar uma instância de dados em uma ou mais classes conhecidas. A classificação de texto é uma instância especial do problema de classificação, onde os dados de entrada são texto e o objetivo é categorizar o pedaço de texto em um ou mais grupos (chamados de classe) a partir de um conjunto de grupos predefinidos (Classes). O “texto” pode ter comprimento arbitrário: um caractere, uma palavra, uma frase, um parágrafo ou um documento completo.\n",
    "\n",
    "Considerando o caso de reviews de filmes, o desafio de classificação de texto é aprender esta categorização a partir de uma coleção de exemplos para cada categoria e predizer a categoria para novos, ainda não vistos, reviews. \n",
    "\n",
    "Todo problema de classificação supervisionado pode ainda ser divido em três partes baseado no número de classes envolvidas:\n",
    "1. binário, quando há apenas duas classes envolvidas; \n",
    "2. multiclasse, quando há mais de duas classes envolvidas, como quando quero classificar sentimentos em negativo, neutro ou positivo, por exemplo; \n",
    "3. multilabel, em que um documento pode ter associado a ele uma ou mais classes, como num artigo de jornal, em que o mesmo pode tratar de política, negócios e jurídico, por exemplo. Em nosso curso, trabalharemos com os dois primeiros casos. \n",
    "\n",
    "Vamos, então, entender o problema de classificação de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8531f4-7eb3-4153-8bcc-ee0d5a592c40",
   "metadata": {},
   "source": [
    "## O Problema de Classificação de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb06131-f3f3-4857-bbb1-799076014653",
   "metadata": {},
   "source": [
    "Vamos definir formalmente o problema de classificação de texto.\n",
    "\n",
    "Seja\n",
    "* $d \\in X$ um documento, em que $X$ é o espaço de documentos\n",
    "* $C=\\{c_1,c_2,\\dots,c_j\\}$ um conjunto fixo de Classes (categorias ou rótulos)\n",
    "* $\\{(d_1,c_1 ),(d_2,c_2 )\\dots,(d_m,c_m )\\}$ um conjunto de treinamento de m documentos manualmente rotulados\n",
    "\n",
    "Usando um algoritmo de aprendizado, desejamos aprender uma função $\\gamma$ de classificação que mapeia documentos para classes:\n",
    "* $\\gamma = X \\rightarrow C$\n",
    "\n",
    "Mas se $d \\in X$ é um documento que pertence ao espaço de documentos, como representa-lo de forma que um computador consiga interpretá-lo? Como visto anteriormente, precisamos transformar esse documento em números. \n",
    "\n",
    "O jeito como eu realizo essa transformação é denominado modelo de representação, e esse modelo criado recebe o nome de vetor de características (nota: podemos usar features e descritores como sinônimos de características). \n",
    "\n",
    "Inicialmente, vamos trabalhar com dois modelos de representação: Bag-of-Words e TF-IDF. São modelos mais simples, mas que oferecem um resultado muito satisfatório para várias aplicações, inclusive veremos um caso prático na aula 5. Vamos começar pelo Bag-of-words. \n",
    "\n",
    "Importante notarmos que a representação que vimos na aula passada, em que colocamos 1 quando a palavra do dicionário está na frase e 0 quando contrário, é um tipo ainda mais simples denominado One-Hot-encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efc925-fada-48b8-8b61-82df7c555366",
   "metadata": {},
   "source": [
    "### Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81049c21-d10c-4ab2-b7cc-447132279abb",
   "metadata": {},
   "source": [
    "O modelo Bag-of-Words (BoW) usa um vetor de contagens de palavras para representar um documento. Observe:\n",
    "\n",
    "$x=[1,0,1,0,4,3,10,6,7,\\dots]$, em que $x_j$ é a contagem da palavra $j$. \n",
    "\n",
    "O tamanho de $x$ é determinado pelo vocabulário $|V|$, que é o conjunto de todas as possíveis palavras no vocabulário. Lembrando que o vocabulário é composto por todas as palavras distintas presentes no conjunto de documentos a serem classificados. \n",
    "\n",
    "A intuição básica por trás disso é que ele assume que o texto pertencente a uma determinada classe no conjunto de dados é caracterizado por um conjunto único de palavras. Se dois trechos de texto tiverem quase as mesmas palavras, então eles pertencem ao mesmo conjunto (classe). Assim, ao analisar as palavras presentes em um trecho de texto, é possível identificar a classe (bag) a que ele pertence.\n",
    "\n",
    "Por conta disso, o BoW somente inclui informação sobre a contagem de cada palavra e não sobre a ordem em que cada uma aparece. Logo, o contexto das frases é ignorado ao criar essa representação. Ainda assim, é surpreendente efetivo para classificação de texto. \n",
    "A imagem a seguir mostra como funciona o BoW:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdafe2-7dbd-48c4-ad5f-4f5f9a2f90b8",
   "metadata": {},
   "source": [
    "<img src=\"img/bow.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a261c5-4a11-4590-a97c-2b03a0758e47",
   "metadata": {},
   "source": [
    "Esse tipo de representação é muito utilizado na maioria das aplicações de NLP. Aqui estão algumas vantagens de se o usar:\n",
    "\n",
    "1. Simples de entender e implementar.\n",
    "\n",
    "2. Documentos que possuam as mesmas palavras terão vetores de representação próximos um do outro no espaço Euclidiano quando comparado com documentos com palavras completamente diferentes. Dessa maneira, o espaço vetorial resultante do BoW consegue capturar a similaridade semântica dos documentos. \n",
    "\n",
    "Entretanto, possui também algumas desvantagens:\n",
    "\n",
    "1. O tamanho do vetor de representação aumenta com o tamanho do vocabulário. Assim, esparsidade continua sendo um problema. \n",
    "\n",
    "2. Ele não consegue capturar a similaridade entre diferentes palavras que possuam o mesmo significado. Por exemplo, considere três documentos: “Eu corro”, “Eu corri”, “Eu comi”. Todos eles estarão igualmente espaçados. \n",
    "\n",
    "3. O BoW não possui uma maneira de lidar com palavras que estejam fora de seu vocabulário. \n",
    "\n",
    "4. A informação da ordem que uma palavra aparece na frase é perdida com essa representação.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d2cdd-ed89-47fa-8e58-90fa9c16b088",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0dcd0e-99d1-47c8-bc7d-aa5c6261db9c",
   "metadata": {},
   "source": [
    "Outro ponto que não comentamos a respeito dos modelos de representação vistos até o momento foi a respeito da importância da palavra num documento. Quando pensamos num texto, claramente algumas palavras possuem uma importância relativa maior em relação às outras. Para conseguir capturar isso, usamos outro tipo de representação, TF-IDF, que significa **Term Frequency – Inverse Document Frequency**. \n",
    "\n",
    "O conceito de frequência de termo (TF) calcula a proporção de um termo num documento em relação ao número total de termos nesse documento. Entretanto, um problema com pontuar frequências de palavras é que palavras muito frequentes começam a dominar no documento (pontuação alta), mas podem não conter muita “informação de conteúdo” para o modelo em relação a palavras raras que pertençam a domínios específicos. Assim, introduzimos um mecanismo para atenuar o efeito de termos que ocorrem muito nos dados para tornar significativo a determinação de sua relevância. Esse mecanismo é o IDF.\n",
    "\n",
    "A intuição por trás do TF-IDF é a seguinte: se uma palavra $w$ aparece muitas vezes em um documento $d_i$, mas não ocorre muito nos demais documentos $d_j$ do corpus, então a palavra $w$ deve ser de grande importância para o documento $d_i$. A importância de $w$ deve aumentar proporcionalmente à sua frequência em $d_i$, mas, ao mesmo tempo, sua importância deve diminuir proporcionalmente à frequência da palavra em outros documentos $d_j$ do corpus. Matematicamente, isso é capturado usando duas quantidades: TF e IDF. Os dois são então combinados para chegar ao score TF-IDF.\n",
    "\n",
    "TF mede a frequência com que um termo ou palavra ocorre em um determinado documento. Como diferentes documentos no corpus podem ter comprimentos diferentes, um termo pode ocorrer com mais frequência em um documento mais longo do que em um documento mais curto. Para normalizar essas contagens, dividimos o número de ocorrências pelo comprimento do documento. Assim, podemos definir TF da seguinte maneira:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0496da-31df-4a0a-81fb-685c36a39308",
   "metadata": {},
   "source": [
    "$tf = \\frac{f_{t,d}}{\\sum{T,d}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106364f-2c1f-4fff-ba48-11769d762f9f",
   "metadata": {},
   "source": [
    "IDF mede a importância do termo em um corpus. No cálculo do TF, todos os termos recebem igual importância (ponderação). No entanto, é um fato bem conhecido que palavras irrelevantes como é, são, sou, etc., não são importantes, embora ocorram com frequência. Para dar conta de tais casos, o IDF pondera para baixo os termos que são muito comuns em um corpus e pondera para cima os termos raros. O IDF é calculado da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1963651-bf5c-4601-af6c-2d8c054218a2",
   "metadata": {},
   "source": [
    "$idf = \\log_e \\frac{N}{n_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c1d45-2eef-4326-a1ed-621024e5d5b4",
   "metadata": {},
   "source": [
    "Por fim, multiplicamos os dois termos, obtendo o score final:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d40160-e16a-491b-97f3-4035204943e7",
   "metadata": {},
   "source": [
    "$TFIDF = tf * idf$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38c23e-5ea2-4f6d-9d62-ec437c373817",
   "metadata": {},
   "source": [
    "Em que:\n",
    "\n",
    "* t: termo analisado\n",
    "* d: documento analisado\n",
    "* T: conjunto de termos presente no documento $d$\n",
    "* N: número total de documentos\n",
    "* $n_t$: número de documentos que contém o termo $t$\n",
    "\n",
    "Semelhante ao BoW, podemos usar os vetores TF-IDF para calcular a similaridade entre dois textos usando uma medida de similaridade como distância euclidiana ou similaridade de cosseno. TF-IDF é uma representação comumente usada em aplicações como recuperação de informação e classificação de texto. No entanto, apesar do TF-IDF ser melhor que os métodos de vetorização que vimos anteriormente em termos de captura de semelhanças entre palavras, ele ainda sofre com a maldição da alta dimensionalidade. Veremos como amenizar esse problema na aula seguinte. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b08e21-4fbe-4b1e-b5ab-1411a1495991",
   "metadata": {},
   "source": [
    "## Prática\n",
    "\n",
    "Vamos analisar o código agora e aplicar esses conhecimentos obtidos num problema de classificação de texto. Para entender melhor o problema de classificação de textos, vamos começar com um exemplo básico que servirá como intuição para nos aprofundarmos depois. Considere o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6787b8df-16ab-4e97-8fc4-92e2a30c2f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sobre postech? Eu gostei muito do postech da FIAP</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O postech da FIAP pode melhorar, não gostei muito</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foi muito importante para meu desenvolvimento</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poderia ser mais técnico. Não gostei</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     class\n",
       "0  Sobre postech? Eu gostei muito do postech da FIAP  positivo\n",
       "1  O postech da FIAP pode melhorar, não gostei muito  negativo\n",
       "2      Foi muito importante para meu desenvolvimento  positivo\n",
       "3               Poderia ser mais técnico. Não gostei  negativo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# exemplo de documento e corpus\n",
    "df_fiap = pd.DataFrame({\n",
    "    'text': [\n",
    "      'Sobre postech? Eu gostei muito do postech da FIAP',\n",
    "      'O postech da FIAP pode melhorar, não gostei muito',\n",
    "      'Foi muito importante para meu desenvolvimento',\n",
    "      'Poderia ser mais técnico. Não gostei'\n",
    "    ],\n",
    "    'class': [\n",
    "      'positivo',\n",
    "      'negativo',\n",
    "      'positivo',\n",
    "      'negativo'\n",
    "    ]})\n",
    "\n",
    "df_fiap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82243e-798c-41b8-a0e8-b8cfd3ea07fb",
   "metadata": {},
   "source": [
    "Esse DataFrame contém duas colunas, uma que mostra um texto, que é um review sobre a Pos-tech da FIAP, e outra que mostra a classe, ou o sentimento, que no caso pode ser positivo ou negativo. \n",
    "\n",
    "Com essa informação, podemos usar as representações que vimos aqui e entender como transformamos texto em número. Vamos usar o BoW nesse primeiro exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6babc074-eb73-4489-99de-5a77c9f5a570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>da</th>\n",
       "      <th>desenvolvimento</th>\n",
       "      <th>do</th>\n",
       "      <th>eu</th>\n",
       "      <th>fiap</th>\n",
       "      <th>foi</th>\n",
       "      <th>gostei</th>\n",
       "      <th>importante</th>\n",
       "      <th>mais</th>\n",
       "      <th>melhorar</th>\n",
       "      <th>meu</th>\n",
       "      <th>muito</th>\n",
       "      <th>não</th>\n",
       "      <th>para</th>\n",
       "      <th>pode</th>\n",
       "      <th>poderia</th>\n",
       "      <th>postech</th>\n",
       "      <th>ser</th>\n",
       "      <th>sobre</th>\n",
       "      <th>técnico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   da  desenvolvimento  do  eu  fiap  foi  gostei  importante  mais  melhorar  meu  muito  não  para  pode  poderia  postech  ser  sobre  técnico\n",
       "0   1                0   1   1     1    0       1           0     0         0    0      1    0     0     0        0        2    0      1        0\n",
       "1   1                0   0   0     1    0       1           0     0         1    0      1    1     0     1        0        1    0      0        0\n",
       "2   0                1   0   0     0    1       0           1     0         0    1      1    0     1     0        0        0    0      0        0\n",
       "3   0                0   0   0     0    0       1           0     1         0    0      0    1     0     0        1        0    1      0        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_fiap.text)\n",
    "text_vect = vect.transform(df_fiap.text)\n",
    "\n",
    "pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d645050-4cdd-4cde-919c-f41dcb3dd4bb",
   "metadata": {},
   "source": [
    "Observe que, por padrão, o método CountVectorizer passa as palavras para minúscula. Essa representação nos mostra o dicionário que foi obtido a partir do conjunto de textos que passamos como parâmetro. Observe que a palavra, também chamado de token, “postech” tem valor 2 na linha 0, pois ela apareceu duas vezes na primeira frase. \n",
    "\n",
    "Podemos comparar essa representação com a obtida usando TF-IDF. Observe o código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c73ff8-e9a9-4580-977f-d3f519a45652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>da</th>\n",
       "      <th>desenvolvimento</th>\n",
       "      <th>do</th>\n",
       "      <th>eu</th>\n",
       "      <th>fiap</th>\n",
       "      <th>foi</th>\n",
       "      <th>gostei</th>\n",
       "      <th>importante</th>\n",
       "      <th>mais</th>\n",
       "      <th>melhorar</th>\n",
       "      <th>meu</th>\n",
       "      <th>muito</th>\n",
       "      <th>não</th>\n",
       "      <th>para</th>\n",
       "      <th>pode</th>\n",
       "      <th>poderia</th>\n",
       "      <th>postech</th>\n",
       "      <th>ser</th>\n",
       "      <th>sobre</th>\n",
       "      <th>técnico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364073</td>\n",
       "      <td>0.364073</td>\n",
       "      <td>0.287039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364073</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277223</td>\n",
       "      <td>0.342426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         da  desenvolvimento        do        eu      fiap       foi    gostei  importante      mais  melhorar       meu     muito       não      para      pode   poderia   postech       ser     sobre   técnico\n",
       "0  0.287039         0.000000  0.364073  0.364073  0.287039  0.000000  0.232383    0.000000  0.000000  0.000000  0.000000  0.232383  0.000000  0.000000  0.000000  0.000000  0.574078  0.000000  0.364073  0.000000\n",
       "1  0.342426         0.000000  0.000000  0.000000  0.342426  0.000000  0.277223    0.000000  0.000000  0.434323  0.000000  0.277223  0.342426  0.000000  0.434323  0.000000  0.342426  0.000000  0.000000  0.000000\n",
       "2  0.000000         0.430037  0.000000  0.000000  0.000000  0.430037  0.000000    0.430037  0.000000  0.000000  0.430037  0.274487  0.000000  0.430037  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "3  0.000000         0.000000  0.000000  0.000000  0.000000  0.000000  0.284626    0.000000  0.445922  0.000000  0.000000  0.000000  0.351570  0.000000  0.000000  0.445922  0.000000  0.445922  0.000000  0.445922"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df_fiap.text)\n",
    "text_vect = vect.transform(df_fiap.text)\n",
    "\n",
    "pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d877ab-7896-4eff-8ec1-84e4f5125679",
   "metadata": {},
   "source": [
    "Esse exemplo nos dá uma ideia de como podemos construir uma representação numérica de textos e inserir esses dados num modelo de machine learning. Vamos agora analisar um exemplo mais complexo.\n",
    "\n",
    "Quando falamos de NLP, é comum escrevermos funções que podem ser usadas em vários contextos, ou seja, o mesmo código acaba servindo para diferentes bases de dados. Assim, uma prática recomendada é ir construindo sua própria biblioteca de funções para poder usá-la sempre que for trabalhar com texto. \n",
    "\n",
    "Na aula passada, vimos uma série de técnicas que usamos para normalizar os textos. Vamos apresentar mais algumas e criar uma biblioteca de normalização de textos para poder aplica-la nos textos do exemplo a seguir.\n",
    "\n",
    "A primeira coisa a ser feita é importar os pacotes que vamos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91553477-b199-4e2f-a22b-1753172abe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0476ef7-e1a0-4df2-9bf3-1b679fbb4949",
   "metadata": {},
   "source": [
    "Depois de importar os pacotes, podemos começar a criar nossas funções. A primeira delas tem como objetivo remover toda acentuação das palavras. Para isso, vamos usar o pacote *unicodedata*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02fd8fd-8c1e-46f1-b34b-4f545cfb9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_accents(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dad5b-f636-4905-8cb1-7295bc2d99aa",
   "metadata": {},
   "source": [
    "Explicação da função: \n",
    "\n",
    "1. unicodedata.normalize(\"NFKD\", text): Aqui, a função normalize do módulo unicodedata é utilizada para normalizar o texto Unicode. A normalização é feita usando o algoritmo NFKD (Forma Normalizada de Compatibilidade Compatível com Decomposição). Isso pode ser útil para tratar caracteres acentuados, caracteres especiais e outras variações de representação Unicode.\n",
    "\n",
    "2. .encode(\"ASCII\", \"ignore\"): O método encode é chamado na string normalizada. Aqui, a string é codificada usando o esquema de codificação ASCII. O parâmetro \"ignore\" especifica que, se houver caracteres na string que não podem ser representados no esquema ASCII, eles serão simplesmente ignorados e excluídos da saída.\n",
    "\n",
    "3. .decode(\"utf-8\"): Após a codificação usando ASCII, o método decode é chamado na sequência de bytes resultante. Aqui, a sequência de bytes é decodificada de volta para uma string usando o esquema de codificação UTF-8.\n",
    "\n",
    "Portanto, o objetivo desse código é transformar um texto Unicode em uma representação que contenha apenas caracteres ASCII, ignorando qualquer caractere que não possa ser representado no esquema ASCII. O uso da normalização NFKD no início pode ajudar a lidar com diferentes formas de caracteres Unicode (por exemplo, caracteres acentuados representados de diferentes maneiras) antes de transformar a string em uma representação ASCII.\n",
    "\n",
    "O significa de “NFKD” é além do escopo dessa disciplina, mas nas referencias bibliográficas, eu deixei um artigo que pode ser consultado para um completo entendimento do assunto. \n",
    "\n",
    "A segunda função faz a remoção da pontuação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b72bea-1e2f-4d65-bf5b-00911ed5a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    table = str.maketrans({key: \" \" for key in punctuations})\n",
    "    text = text.translate(table)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9ff22-58cf-4ce1-abd2-6f91c61dfcbf",
   "metadata": {},
   "source": [
    "Explicação da função: \n",
    "\n",
    "1. punctuations = string.punctuation: Aqui, a variável punctuations é atribuída ao conjunto de caracteres de pontuação predefinidos do módulo string. Esses caracteres incluem coisas como pontos, vírgulas, ponto e vírgulas, exclamações, interrogações e assim por diante.\n",
    "\n",
    "2. table = str.maketrans({key: \" \" for key in punctuations}): A função str.maketrans é usada para criar uma tabela de tradução que substitui cada caractere de pontuação por um espaço em branco. Um dicionário de compreensão é usado para criar esse dicionário de tradução, onde as chaves são os caracteres de pontuação e os valores são espaços em branco.\n",
    "\n",
    "3. text = text.translate(table): O método translate é chamado na string text, usando a tabela de tradução criada na etapa anterior. Isso substituirá cada caractere de pontuação na string por um espaço em branco, efetivamente removendo a pontuação.\n",
    "\n",
    "4. return text: A função retorna a string resultante após a remoção da pontuação.\n",
    "\n",
    "Portanto, a função remove_punctuation remove todos os caracteres de pontuação de uma string de entrada, substituindo-os por espaços em branco, e retorna a versão da string sem a pontuação. \n",
    "\n",
    "A próxima função é de normalização do texto. Ela agrega as duas funções já vistas e mais algumas operações. Vamos entende-la: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613368fa-6825-4ec9-b8b3-dddfdbb47fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_str(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = normalize_accents(text)\n",
    "    text = re.sub(re.compile(r\" +\"), \" \",text)\n",
    "    return \" \".join([w for w in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9db910-ff31-460e-9a61-2af4b066960c",
   "metadata": {},
   "source": [
    "Explicação da função:\n",
    "\n",
    "1. text = text.lower(): a função recebe um texto como parâmetro e passa esse texto para minúscula;\n",
    "\n",
    "2. text = remove_punctuation(text): aplica a função descrita acima, removendo a pontuação do texto\n",
    "\n",
    "3. text = normalize_accents(text): aplica a função descrita acima, removendo acentuação\n",
    "\n",
    "4. text = re.sub(re.compile(r” +”), “ ”,text): elemina qualquer espaço em branco extra que possa existir no texto\n",
    "\n",
    "5. no retorno, a string é reorganizada novamente, eliminando qualquer espaço que possa ter sido gerado pelas etapas anteriores. \n",
    "\n",
    "Por fim, temos uma função que agrega todas as demais para aplicar todas elas de uma vez num texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1da0441-e46e-4a2a-af10-88329b7f0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\") # portuguese, caso o dataset seja em português\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)\n",
    "        text = \"\".join([w for w in text if not w.isdigit()])\n",
    "        text = word_tokenize(text)\n",
    "        text = [x for x in text if x not in stop_words]\n",
    "        text = [y for y in text if len(y) > 2]\n",
    "        return \" \".join([t for t in text])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16038c-d1a0-46a1-8c51-1c8b02188504",
   "metadata": {},
   "source": [
    "Explicação da função: \n",
    "\n",
    "1. stop_words = nltk.corpus.stopwords.words(\"english\"): é carregado um conjunto de stop words e armazenado na variável stop_words. Caso o dataset seja em português, é só alterar o parâmetro para “portuguese”. \n",
    "\n",
    "2. If isinstance(text, str): verifica se o tipo da variável text é str. Se não for, retorna None. Se for, realiza os demais passos, a saber:\n",
    "\n",
    "    a. Text = normalize_str(text): aplica a função acima descrita\n",
    "    \n",
    "    b. text = \"\".join([w for w in text if not w.isdigit()]): remove qualquer número da string\n",
    "    \n",
    "    c. text = word_tokenize(text): divide a string em tokens (palavras)\n",
    "    \n",
    "    d. text = [x for x in text if x not in stop_words]: elimina qualquer stopwords da string\n",
    "    \n",
    "    e. text = [y for y in text if len(y) > 2]: mantém apenas strings que tenham mais que 2 caracteres. NOTA: este passo é opcional e eu apliquei pois fazia sentido para o dataset que vamos trabalhar. \n",
    "    \n",
    "    f.return \" \".join([t for t in text]): retorna a string tratada. \n",
    "    \n",
    "Vamos ver um exemplo de aplicação dessa função: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7784fed-ee77-4872-9ce1-907b7109cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exemplo normalizacao'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Exemplo$ de 12 normalização!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20317a-811b-4d40-8cd7-3b474d106e2f",
   "metadata": {},
   "source": [
    "Vamos agora usar essas funções que criamos num dataset real e entender como realizar o pré-processamento de textos bem como preparar os dados para treinar um modelo de machine learning.\n",
    "\n",
    "Para isso, vamos usar o dataset UCI News Aggregator, que pode ser consultado nesse [link](https://archive.ics.uci.edu/dataset/359/news+aggregator).\n",
    "\n",
    "A primeira coisa a ser feita é ler o arquivo CSV e filtrar somente o título da notícia bem como a categoria a qual ela pertence. Esse caso é típico problema de classificação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "152ae2b0-c8ba-4abb-8528-646ffaa6b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bases/uci-news-aggregator.csv')\n",
    "df = df[['TITLE','CATEGORY']]\n",
    "#categories: b = business, t = science and technology, e = entertainment, m = health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a1b44-0cbb-401f-81c3-1b707e9e119d",
   "metadata": {},
   "source": [
    "Agora, precisamos embaralhar os dados. Com isso, evitamos que o modelo aprenda bem somente sobre uma classe, já que ele pode ficar preso em mínimos locais. Para isso, usaremos o método Shuffle, da biblioteca utils da Scikit-Learn. Por fim, reiniciamos o index e eliminamos a nova coluna de índice criada e mostramos as 5 primeiras linhas de nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50ea447-8f70-4fc7-a818-bc1ccce855c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PBOC widening yuan band aggravating concerns a...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US Close: Stocks slide as Fed officials talk a...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To play or not to play? Corporate sponsorship,...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zac Efron and Seth Rogan dress in drag on The ...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Doctor Who trailer hints at Daleks' return</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  PBOC widening yuan band aggravating concerns a...        b\n",
       "1  US Close: Stocks slide as Fed officials talk a...        b\n",
       "2  To play or not to play? Corporate sponsorship,...        e\n",
       "3  Zac Efron and Seth Rogan dress in drag on The ...        e\n",
       "4     New Doctor Who trailer hints at Daleks' return        e"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df = df.reset_index(drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72c3c1-b24d-4f4a-a29d-af618f6bcae8",
   "metadata": {},
   "source": [
    "Agora, precisamos aplicar as funções que escrevemos anteriormente para tratar o texto de todas as linhas da coluna Title. Para isso, usamos o código a seguir, que cria uma nova coluna com o texto tratado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03bbc45d-ce0d-4e0d-ae87-a11cc88b23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Treated'] = df['TITLE'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfedf9e-01d9-463f-8feb-eaf3df032b7d",
   "metadata": {},
   "source": [
    "Podemos ver um exemplo comparando o texto antes e depois do tratamento. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f064d9e4-3996-4efd-aa92-cba627b1780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes:  PBOC widening yuan band aggravating concerns about exposure to China in  ... \n",
      "\n",
      "Depois:  pboc widening yuan band aggravating concerns exposure china\n"
     ]
    }
   ],
   "source": [
    "print('Antes: ', df['TITLE'][0], '\\n')\n",
    "print('Depois: ', df['Title_Treated'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d97d4-97e1-4f35-8ba6-31afc2de2c59",
   "metadata": {},
   "source": [
    "Agora, vamos preparar esses dados para usá-los para treinar um modelo de machine learning.\n",
    "\n",
    "Primeiro, importamos o método train_test_split, que pega nossa base de dados e a separa em conjunto de treino e teste. Para isso, precisamos definir quais são as features que o modelo vai usar para treinar e qual a resposta associada a cada amostra. As features serão colocadas na variável X e a classe será posta na variável y. Após isso, o método train_test_split é chamado, criando quatro conjuntos:\n",
    "\n",
    "1. X_train são as features referentes ao conjunto de treino\n",
    "2. y_train é a classe de cada amostra do conjunto de treino\n",
    "3. X_teste são as features referentes ao conjunto de teste\n",
    "4. y_teste é a classe de cada amostra do conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5366b4b-03b9-4ed8-922b-5e054949da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Title_Treated'].values\n",
    "y = df['CATEGORY'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4e854-0b73-4c39-80d6-05a4d994730d",
   "metadata": {},
   "source": [
    "Agora, criamos uma representação usando o BoW, baseado no mesmo código que vimos até agora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0320627f-76a1-4230-936d-f9250319eda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<295693x43672 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1953706 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(lowercase=False) \n",
    "vect.fit(X_train)\n",
    "X_train = vect.transform(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97eeed-fc91-4dc8-b271-0c7d2ba253e7",
   "metadata": {},
   "source": [
    "Observe que o retorno foi uma matriz esparsa. O primeiro valor é a quantidade de amostras que eu tenho no conjunto de treino. O segundo valor é o tamanho do dicionário que foi aprendido durante o processo de treino. \n",
    "\n",
    "Agora, precisamos pegar esse dicionário aprendido e aplica-lo ao conjunto de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50419424-9d72-4371-8f2a-b4580f0ff3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<126726x43672 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 832787 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vect.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f8f88-8271-4d7c-8500-fe8f5e8a2fce",
   "metadata": {},
   "source": [
    "Novamente, o primeiro valor é a quantidade de amostras presente no conjunto de teste, agora. E o segundo valor é o tamanho do dicionário aprendido durante o processo de treino. \n",
    "\n",
    "Agora, importamos um modelo de machine learning e vamos usar os dados transformados para treinar o modelo e, depois, testar e ver o quão bem nosso modelo foi. Para esse caso, vamos usar o SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73396738-8be1-4cff-b132-01b6bee218b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempo decorrido:  3304.206557035446 segundos\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear') \n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print('tempo decorrido: ',end_time-start_time, 'segundos')\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76dc12d9-2adc-4c49-87e1-7025d17762a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:55:04.206557\n"
     ]
    }
   ],
   "source": [
    "#traduzindo o tempo decorrido \n",
    "import datetime\n",
    "sec = end_time-start_time\n",
    "print(str(datetime.timedelta(seconds = sec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30a9c3-6c9e-4c22-9085-ac900c8ca567",
   "metadata": {},
   "source": [
    "Primeiro, eu instancio o SVM usando o kernel linear. O método fit() é o responsável por treinar o modelo e o método predict() usa o modelo aprendido e realiza a predicao das amostras no conjunto de teste. \n",
    "\n",
    "Por fim, podemos verificar o quanto nosso modelo acertou no conjunto de teste. O método accuracy_score retorna a acurácia para o conjunto de teste. Veja o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b90950f9-04dd-436b-baa3-ca0f2f02f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9439341571579628\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ec92c-a972-42db-ac58-b92ece6f1534",
   "metadata": {},
   "source": [
    "## Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cd7ae-0eff-4b91-81ea-92b2c55cda6a",
   "metadata": {},
   "source": [
    "Análise de sentimentos é uma aplicação de classificação de textos. Geralmente, separamos os sentimentos em três classes: negativo, neutro e positivo. Como exercício de aplicação, temos uma base contendo tweets e nosso objetivo será o de criar um classificador que diga se o tweet analisado tem sentimento negativo, neutro ou positivo. \n",
    "\n",
    "O jupyter notebook desse exercício contém as orientações necessárias para guia-lo na construção desse classificador. Hora de colocar em prática o que você viu até o momento. \n",
    "\n",
    "Recomendamos que você tente resolver todos os exercícios sem olhar o jupyter com as respostas para avaliar seu conhecimento. Depois, compare as respostas e nos chame no discord para discutirmos as soluções. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689e776-925f-408e-b775-62ed97ce842a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
