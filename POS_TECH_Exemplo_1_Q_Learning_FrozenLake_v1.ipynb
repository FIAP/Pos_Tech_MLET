{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neste notebook, voc√™ codificar√° do zero seu primeiro agente de Reinforcement Learning jogando FrozenLake ‚ùÑÔ∏è usando Q-Learning"
      ],
      "metadata": {
        "id": "clnLGkknvrG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptado HuggingFace"
      ],
      "metadata": {
        "id": "tQFWPAzi0tbk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://www.gymlibrary.dev/_images/frozen_lake.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üéÆ Environments:\n",
        "\n",
        ">\n",
        "\n",
        "- [FrozenLake-v1](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)\n",
        "\n",
        "\n",
        "###üìö RL-Library:\n",
        "\n",
        "- Python and NumPy\n",
        "- [Gym](https://www.gymlibrary.dev/)"
      ],
      "metadata": {
        "id": "DPTBOv9HYLZ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## Pequena revis√£o de Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "- O *Q-Learning* **√© o algoritmo RL que**\n",
        "\n",
        "   - Treina *Q-Function*, uma **fun√ß√£o a√ß√£o-valor (action-value function)** que cont√©m, como mem√≥ria interna, uma *Q-table* **que cont√©m todos os valores do par estado-a√ß√£o.**\n",
        "\n",
        "   - Dado um estado e uma a√ß√£o, nossa Q-Function **pesquisar√° em sua Q-table o valor correspondente.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
        "\n",
        "- Quando o treinamento √© conclu√≠do,**temos uma Fun√ß√£o-Q ideal, portanto, uma Tabela-Q ideal.**\n",
        "\n",
        "- E se **tivermos uma fun√ß√£o Q √≥tima**,\n",
        "ter uma pol√≠tica ideal, pois **sabemos para cada estado qual √© a melhor a√ß√£o a ser tomada.**\n",
        "\n",
        "Mas, no come√ßo, nossa **Q-Table √© in√∫til, pois fornece um valor arbitr√°rio para cada par estado-a√ß√£o (na maioria das vezes, inicializamos a Q-Table com valores 0)**. Mas, conforme vamos explorando o ambiente e atualizando nosso Q-Table, ele nos dar√° aproxima√ß√µes cada vez melhores\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
        "\n",
        "This is the Q-Learning pseudocode:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's code our first Reinforcement Learning algorithm üöÄ"
      ],
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar depend√™ncias e criar um display virtual üîΩ\n"
      ],
      "metadata": {
        "id": "4gpxC1_kqUYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.24\n",
        "!pip install pygame\n",
        "!pip install numpy\n",
        "\n",
        "!pip install pickle5\n",
        "!pip install pyyaml==6.0\n",
        "!pip install imageio\n",
        "!pip install imageio_ffmpeg\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install ffmpeg xvfb\n",
        "!pip install xvfbwrapper\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "n71uTX7qqzz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para garantir que as novas bibliotecas instaladas sejam usadas, **√†s vezes √© necess√°rio reiniciar o tempo de execu√ß√£o do notebook**. A pr√≥xima c√©lula for√ßar√° o **tempo de execu√ß√£o a travar, ent√£o voc√™ precisar√° se conectar novamente e executar o c√≥digo a partir daqui**."
      ],
      "metadata": {
        "id": "K6XC13pTfFiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Importa√ß√£o de pacotes üì¶\n",
        "\n",
        "Al√©m das bibliotecas instaladas, utilizamos tamb√©m:\n",
        "\n",
        "- `random`: Para gerar n√∫meros aleat√≥rios (que ser√£o √∫teis para a pol√≠tica epsilon-greedy).\n",
        "- `imageio`: Para gerar um v√≠deo de replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp4-bXKIy1mQ"
      },
      "source": [
        "We're now ready to code our Q-Learning algorithm üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Part 1: Frozen Lake ‚õÑ (non slippery version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## Criando o ambiente FrozenLake ‚õÑ (https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)\n",
        "---\n",
        "\n",
        "üí° Um bom h√°bito quando voc√™ come√ßa a usar um ambiente √© verificar sua documenta√ß√£o\n",
        "\n",
        "üëâ https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
        "\n",
        "---\n",
        "\n",
        "Vamos treinar nosso agente de Q-Learning **para navegar do estado inicial (S) para o estado objetivo (G) andando apenas em ladrilhos congelados (F) e evitando buracos (H)**.\n",
        "\n",
        "Podemos ter dois tamanhos de ambiente:\n",
        "\n",
        "- `map_name=\"4x4\"`: uma vers√£o de grade 4x4\n",
        "- `map_name=\"8x8\"`: uma vers√£o em grade 8x8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "Por enquanto vamos simplificar com o mapa 4x4 e antiderrapante (is_slippery=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### Vamos ver como fica o Environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "Vemos com a sa√≠da `Observation Space Shape Discrete(16)` que a observa√ß√£o √© um inteiro representando a **posi√ß√£o atual do agente como current_row * nrows + current_col (onde tanto a linha quanto a coluna come√ßam em 0)**.\n",
        "\n",
        "Por exemplo, a posi√ß√£o do objetivo no mapa 4x4 pode ser calculada da seguinte forma: 3 * 4 + 3 = 15. O n√∫mero de observa√ß√µes poss√≠veis depende do tamanho do mapa. **Por exemplo, o mapa 4x4 tem 16 observa√ß√µes poss√≠veis.**\n",
        "\n",
        "\n",
        "Por exemplo, √© assim que estado = 0 se parece:\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "O espa√ßo de a√ß√£o (o conjunto de a√ß√µes poss√≠veis que o agente pode realizar) √© discreto com 4 a√ß√µes dispon√≠veis üéÆ:\n",
        "- 0: V√Å PARA A ESQUERDA\n",
        "- 1: DESCER\n",
        "- 2: V√Å PARA A DIREITA\n",
        "- 3: SUBIR\n",
        "\n",
        "Fun√ß√£o de recompensa üí∞:\n",
        "- Atingir meta: +1\n",
        "- Furo de alcance: 0\n",
        "- Alcance congelado: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Criar e Inicializar a Q-table üóÑÔ∏è\n",
        "(üëÄ Step 1 of the pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "\n",
        "√â hora de inicializar nossa Q-table! Para saber quantas linhas (estados) e colunas (a√ß√µes) usar, precisamos conhecer o espa√ßo de a√ß√£o e observa√ß√£o. J√° conhecemos seus valores anteriormente, mas queremos obt√™-los programaticamente para que nosso algoritmo generalize para diferentes ambientes. Gym nos fornece uma maneira de fazer isso: `env.action_space.n` e `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Vamos criar nossa Qtable de tamanho (state_space, action_space) e inicializar cada valor em 0 usando np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0WlgkVO3Jf9"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake"
      ],
      "metadata": {
        "id": "erBo5T1-BZVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## Defina a pol√≠tica gananciosa ü§ñ\n",
        "Lembre-se de que temos duas pol√≠ticas, pois o Q-Learning √© um algoritmo **off-policy**. Isso significa que estamos usando uma **pol√≠tica diferente para atuar e atualizar a fun√ß√£o de valor**.\n",
        "\n",
        "- Pol√≠tica Epsilon-gananciosa (pol√≠tica de atua√ß√£o)\n",
        "- Greedy-policy (pol√≠tica de atualiza√ß√£o)\n",
        "\n",
        "A pol√≠tica gananciosa tamb√©m ser√° a pol√≠tica final que teremos quando o agente Q-learning for treinado. A pol√≠tica gulosa √© usada para selecionar uma a√ß√£o da tabela Q.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "##Defina a pol√≠tica gananciosa de epsilon ü§ñ\n",
        "\n",
        "Epsilon-greedy √© a pol√≠tica de treinamento que lida com a troca de explora√ß√£o/explora√ß√£o.\n",
        "\n",
        "A ideia com epsilon-greedy:\n",
        "\n",
        "- Com *probabilidade 1‚Ää-‚Ää…õ* : **fazemos exploitation** (ou seja, nosso agente seleciona a a√ß√£o com o maior valor do par estado-a√ß√£o).\n",
        "\n",
        "- Com *probabilidade …õ*: fazemos **exploration** (tentativa de a√ß√£o aleat√≥ria).\n",
        "\n",
        "E √† medida que o treinamento avan√ßa, progressivamente **reduzimos o valor do epsilon, pois precisaremos de cada vez menos exploration e mais exploitation.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Gera aleatoriamente um n√∫mero entre 0 e 1\n",
        "  random_int = random.uniform(0,1)\n",
        "  # if random_int > maior que epsilon --> exploitation\n",
        "  if random_int > epsilon:\n",
        "     # Execute a a√ß√£o com o maior valor dado um estado\n",
        "     # np.argmax pode ser √∫til aqui\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Definindo os hiperpar√¢metros ‚öôÔ∏è\n",
        "Os hiperpar√¢metros relacionados √† explora√ß√£o s√£o alguns dos mais importantes.\n",
        "\n",
        "- Precisamos garantir que nosso agente **explore o espa√ßo de estados** o suficiente para aprender uma boa aproxima√ß√£o de valor. Para fazer isso, precisamos ter decaimento progressivo do epsilon.\n",
        "- Se voc√™ diminuir o epsilon muito r√°pido (decay_rate muito alto), **voc√™ corre o risco de que seu agente fique preso**, j√° que seu agente n√£o explorou o espa√ßo de estado o suficiente e, portanto, n√£o pode resolver o problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Par√¢metros de treinamento\n",
        "n_training_episodes = 10000 # Total de epis√≥dios de treinamento\n",
        "learning_rate = 0.7 # Taxa de aprendizado\n",
        "\n",
        "# Par√¢metros de avalia√ß√£o\n",
        "n_eval_episodes = 100 # N√∫mero total de epis√≥dios de teste\n",
        "\n",
        "# Par√¢metros do ambiente\n",
        "env_id = \"FrozenLake-v1\" # Nome do ambiente\n",
        "max_steps = 99 # Max passos por epis√≥dio\n",
        "gamma = 0.95 # Taxa de desconto\n",
        "eval_seed = [] # A semente de avalia√ß√£o do ambiente\n",
        "\n",
        "# Par√¢metros de explora√ß√£o\n",
        "max_epsilon = 1.0 # Probabilidade de explora√ß√£o no in√≠cio\n",
        "min_epsilon = 0.05 # Probabilidade m√≠nima de explora√ß√£o\n",
        "decay_rate = 0.0005 # Taxa de decaimento exponencial para prob de explora√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## Rotina de Treinamento\n",
        "\n",
        "O loop de treinamento √© assim:\n",
        "```\n",
        "Por epis√≥dio no total de epis√≥dios de treino:\n",
        "\n",
        "Reduza o epsilon (j√° que precisamos cada vez menos de explora√ß√£o)\n",
        "Redefinir o ambiente\n",
        "\n",
        "   Para passo em passos de tempo m√°ximo:\n",
        "     Escolha a a√ß√£o At usar a pol√≠tica gananciosa do epsilon\n",
        "     Tome a a√ß√£o (a) e observe o(s) estado(s) resultante(s) e a recompensa (r)\n",
        "     Atualize o valor Q Q(s,a) usando a equa√ß√£o de Bellman Q(s,a) + lr [R(s,a) + gama * max Q(s',a') - Q(s,a)]\n",
        "     Se terminar, termine o epis√≥dio\n",
        "     Nosso pr√≥ximo estado √© o novo estado\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # # Reduzir epsilon (porque precisamos cada vez menos exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Redefinir o ambiente\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repete\n",
        "    for step in range(max_steps):\n",
        "      # Escolha a a√ß√£o At para usar a pol√≠tica gananciosa (greedy policy) do epsilon\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Treinando o agente Q-Learning üèÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Let's see what our Q-Learning table looks like now üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## Avalia√ß√£o do M√©todo üìù\n",
        "\n",
        "- Definimos o m√©todo de avalia√ß√£o que vamos usar para testar nosso agente Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "   Avalie o agente para epis√≥dios ``n_eval_episodes`` e retorne recompensa m√©dia e padr√£o de recompensa.\n",
        "   :param env: O ambiente de avalia√ß√£o\n",
        "   :param n_eval_episodes: N√∫mero de epis√≥dios para avaliar o agente\n",
        "   :param Q: A tabela Q\n",
        "   :param seed: A matriz de sementes de avalia√ß√£o (para taxi-v3)\n",
        "   \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Avaliando nosso agente Q-Learning üìà\n",
        "\n",
        "- Normalmente, voc√™ deve ter uma recompensa m√©dia de 1,0\n",
        "- O **ambiente √© relativamente f√°cil** j√° que o espa√ßo de estados √© muito pequeno (16). O que voc√™ pode tentar fazer √© [substitu√≠-lo pela vers√£o escorregadia](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), que introduz estocasticidade, tornando o ambiente mais complexo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "   Gerar um v√≠deo de replay do agente\n",
        "   :param env\n",
        "   :param Qtable: Qtable do nosso agente\n",
        "   :param out_directory\n",
        "   :param fps: quantos quadros por segundo (com taxi-v3 e frozenlake-v1 usamos 1)\n",
        "   \"\"\"\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, done, info = env.step(action) # Colocamos diretamente next_state = state para a l√≥gica de grava√ß√£o\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path =  \"replay.mp4\"\n",
        "record_video(env, Qtable_frozenlake, video_path, 0.5)"
      ],
      "metadata": {
        "id": "PKrxycyWylq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "\n",
        "# Show video\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "-R0xIArYz8JG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Ji_UrI5l2zzn",
        "67OdoKL63eDD",
        "B2_-8b8z5k54"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}