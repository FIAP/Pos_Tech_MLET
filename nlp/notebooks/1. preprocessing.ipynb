{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "### Pré processamento de texto\n",
    "<b>Objetivo: </b> Entender porque preprocessamentos são importantes para tarefas envolvendo texto e como limpar o texto.\n",
    "<br><b>Autora:</b> Renata Gotler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao longo da disciplina de NLP (Natural Language Preprocessing), estaremos usando os dados disponibilizados da americanas. Esse dataset contém as informações das avaliações dos produtos, como comentário, número de estrelas e mais. \n",
    "<br> Nessa primeira aula, vamos explorar o texto das avaliações, entender o porque a limpeza dos dados é importante e como realizá-la. Dessa forma, nessa aula focaremos nos comentários sobre os produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6731/3631190906.py:3: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_reviews = pd.read_csv(url)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/main/B2W-Reviews01.csv'\n",
    "\n",
    "df_reviews = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>site_category_lv1</th>\n",
       "      <th>site_category_lv2</th>\n",
       "      <th>review_title</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_birth_year</th>\n",
       "      <th>reviewer_gender</th>\n",
       "      <th>reviewer_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:11:28</td>\n",
       "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
       "      <td>132532965</td>\n",
       "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Informática</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>Bom</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>F</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:13:48</td>\n",
       "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
       "      <td>22562178</td>\n",
       "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utilidades Domésticas</td>\n",
       "      <td>Copos, Taças e Canecas</td>\n",
       "      <td>Preço imbatível, ótima qualidade</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>M</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:26:02</td>\n",
       "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
       "      <td>113022329</td>\n",
       "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
       "      <td>philips walita</td>\n",
       "      <td>Eletroportáteis</td>\n",
       "      <td>Panela Elétrica</td>\n",
       "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>M</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:35:54</td>\n",
       "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
       "      <td>113851581</td>\n",
       "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
       "      <td>roma jensen</td>\n",
       "      <td>Brinquedos</td>\n",
       "      <td>Veículos de Brinquedo</td>\n",
       "      <td>presente mais que desejado</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>F</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:28</td>\n",
       "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
       "      <td>131788803</td>\n",
       "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
       "      <td>lg</td>\n",
       "      <td>TV e Home Theater</td>\n",
       "      <td>TV</td>\n",
       "      <td>Sem duvidas, excelente</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>M</td>\n",
       "      <td>MG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       submission_date                                        reviewer_id  \\\n",
       "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
       "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
       "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
       "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
       "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
       "\n",
       "  product_id                                       product_name  \\\n",
       "0  132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
       "1   22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
       "2  113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
       "3  113851581               Betoneira Columbus - Roma Brinquedos   \n",
       "4  131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
       "\n",
       "    product_brand      site_category_lv1       site_category_lv2  \\\n",
       "0             NaN            Informática                Notebook   \n",
       "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
       "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
       "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
       "4              lg      TV e Home Theater                      TV   \n",
       "\n",
       "                       review_title  overall_rating recommend_to_a_friend  \\\n",
       "0                               Bom               4                   Yes   \n",
       "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
       "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
       "3        presente mais que desejado               4                   Yes   \n",
       "4            Sem duvidas, excelente               5                   Yes   \n",
       "\n",
       "                                         review_text  reviewer_birth_year  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...               1958.0   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...               1996.0   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               1984.0   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               1985.0   \n",
       "4  A entrega foi no prazo, as americanas estão de...               1994.0   \n",
       "\n",
       "  reviewer_gender reviewer_state  \n",
       "0               F             RJ  \n",
       "1               M             SC  \n",
       "2               M             SP  \n",
       "3               F             SP  \n",
       "4               M             MG  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo podemos ver que:\n",
    "- Temos um total de 132.373 avaliações. No entanto, review_text possui nulos, vamos começar tirando essas avaliações sem texto, visto que não podemos classificar sentimento de um texto inexistente. \n",
    "- Overall rating já está como inteiro, então não precisamos transformar essa coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132373 entries, 0 to 132372\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   submission_date        132373 non-null  object \n",
      " 1   reviewer_id            132373 non-null  object \n",
      " 2   product_id             132373 non-null  object \n",
      " 3   product_name           132289 non-null  object \n",
      " 4   product_brand          40982 non-null   object \n",
      " 5   site_category_lv1      132367 non-null  object \n",
      " 6   site_category_lv2      128360 non-null  object \n",
      " 7   review_title           132071 non-null  object \n",
      " 8   overall_rating         132373 non-null  int64  \n",
      " 9   recommend_to_a_friend  132355 non-null  object \n",
      " 10  review_text            129098 non-null  object \n",
      " 11  reviewer_birth_year    126389 non-null  float64\n",
      " 12  reviewer_gender        128237 non-null  object \n",
      " 13  reviewer_state         128382 non-null  object \n",
      "dtypes: float64(1), int64(1), object(12)\n",
      "memory usage: 14.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews[[\"review_text\", \"overall_rating\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo já conseguimos ter uma ideia do que nos espera, mistura de letras maiúsculas com minúsculas, cada pessoa com um estilo de escrita diferente. O que mais podemos esperar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>overall_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  overall_rating\n",
       "0  Estou contente com a compra entrega rápida o ú...               4\n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...               4\n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               4\n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               4\n",
       "4  A entrega foi no prazo, as americanas estão de...               5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos de machine learning entendem somente números, não texto. Dessa forma, precisamos criar representações vetoriais numéricas para que o algoritmo aprende padrões e consiga fazer inferências sobre dados novos. \n",
    "<br>O primeiro passo para isso é quebrarmos o texto em unidades menores, chamadas tokens. Podemos realizar essa operação de várias formas: por caracter, palavra ou subpalavra, por exemplo.\n",
    "<br><b> O principal objetivo da tokenização é criar um vocabulário mínimo que reduza a quantidade de palavras desconhecidas.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização\n",
    "Vamos começar um exercício, quebrando o nosso texto de diversas formas diferentes e comparando o vocabulário que teríamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from src.utils import count_tokens\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"rslp\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vocab = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenização por caracter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer a separação mais simples de todas: por caracter, isso reduziria bastante nosso vocabulário, certo? Temos um máximo que podemos atingir, que é a quantidade de letras no alfabeto * 2 (maiúscula e minúscula) + variações de acentuação + pontuação + caracteres especiais, mas não vai tão além disso... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [E, s, t, o, u,  , c, o, n, t, e, n, t, e,  , ...\n",
       "1         [P, o, r,  , a, p, e, n, a, s,  , R, $, 1, 9, ...\n",
       "2         [S, U, P, E, R, A,  , E, M,  , A, G, I, L, I, ...\n",
       "3         [M, E, U,  , F, I, L, H, O,  , A, M, O, U, !, ...\n",
       "4         [A,  , e, n, t, r, e, g, a,  , f, o, i,  , n, ...\n",
       "                                ...                        \n",
       "132368    [V, a, l, e,  , m, u, i, t, o, ,,  , e, s, t, ...\n",
       "132369    [P, r, á, t, i, c, o,  , e,  , b, a, r, a, t, ...\n",
       "132370    [C, h, e, g, o, u,  , a, n, t, e, s,  , d, o, ...\n",
       "132371    [M, a, t, e, r, i, a, l,  , f, r, a, c, o, ,, ...\n",
       "132372    [C, o, m, p, r, e, i,  , e, s, s, e,  , p, r, ...\n",
       "Name: token_caracter, Length: 129098, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"token_caracter\"] = df_reviews[\"review_text\"].apply(lambda x: list(x))\n",
    "df_reviews[\"token_caracter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o espaço foi o caracter com maior frequência, seguido por vogais, as menos frequentes foram símbolos, o que faz bastante sentido. No total, nosso vocabulário ficou com tamanho 212."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ', 2892807), ('e', 1638340), ('o', 1554108), ('a', 1504508), ('r', 891104)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caracter_counter = count_tokens(df=df_reviews, token_col=\"token_caracter\")\n",
    "count_vocab[\"caracter_counter\"] = len(caracter_counter)\n",
    "caracter_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('{', 1), ('}', 1), ('͜', 1), ('ʖ', 1), ('ķ', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caracter_counter.most_common()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos olhar esses caracteres especiais? Aparentemente aqule caracter especial ķ foi um erro de digitação no meio dos k que representam uma risada, enquanto o ʖ é o nariz de um rosto. Começamos a perceber melhor porque limpar os dados, não é mesmo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ótimo produto.. Indico para todos! Muito show.  Kkkkkķkkkkk'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[df_reviews[\"review_text\"].str.contains('ķ')][\"review_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gostei muito, sempre que estou em perigo Tony vem me salvar com aquela linda musica do AC/DC. Recomendo muito este produto. Aquela armadura faz horrores gente ( ͡° ͜ʖ ͡°)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[df_reviews[\"review_text\"].str.contains('ʖ')][\"review_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um excelente número de vocabulário, no entanto você já deve saber o problema em dividir o texto dessa forma: é difícil de extrair padrões, relações entre as palavras, porque primeiro o algoritmo vai ter que aprender a \"remontar\" a palavra. A letra por si só não trás informação, com isso, perdemos muita informação. É por isso que essa forma de tokenização não é muito usada na prática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenização por palavra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma bastante usada de tokenização é por palavra, conseguimos extrair padrões do texto através delas. Vamos ver na prática?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [Estou, contente, com, a, compra, entrega, ráp...\n",
       "1         [Por, apenas, R, $, 1994.20, ,, eu, consegui, ...\n",
       "2         [SUPERA, EM, AGILIDADE, E, PRATICIDADE, OUTRAS...\n",
       "3         [MEU, FILHO, AMOU, !, PARECE, DE, VERDADE, COM...\n",
       "4         [A, entrega, foi, no, prazo, ,, as, americanas...\n",
       "                                ...                        \n",
       "132368    [Vale, muito, ,, estou, usando, no, controle, ...\n",
       "132369    [Prático, e, barato, ,, super, indico, o, prod...\n",
       "132370    [Chegou, antes, do, prazo, previsto, e, corres...\n",
       "132371    [Material, fraco, ,, poderia, ser, melhor, ., ...\n",
       "132372    [Comprei, esse, produto, ,, quando, chegou, es...\n",
       "Name: token_word, Length: 129098, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"token_word\"] = df_reviews[\"review_text\"].apply(lambda x: word_tokenize(x))\n",
    "df_reviews[\"token_word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver abaixo que nosso vocabulário aumentou significativamente, certo? De 212 fomos para 75.717! Cada variação teve seu próprio token, temos um token para A, outro para a, outro para as, outro para AS, e assim por diante, mas todos esses tokens tem o mesmo significado, podemos reduzir nosso vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 75717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 175846), ('.', 175179), ('e', 95567), ('o', 82453), ('de', 80880)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter = count_tokens(df=df_reviews, token_col=\"token_word\")\n",
    "count_vocab[\"word_counter\"] = len(word_counter)\n",
    "word_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NY', 1), ('largam', 1), ('21kilis', 1), ('1metro', 1), ('fortinha', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenização por subpalavra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se pudessemos fazer um intermediário entre a tokenização por caracter e por palavra? Podemos reduzir o vocabulário sem perder tanta informação usando a tokenização por subpalavra, que é bastante utilizada em modelos pré-treinados mais complexos, como vamos ver nas próximas aulas. Nessa aula vamos mostrar como exemplo a tokenização por subpalavra usada no Bert, que é a WordPiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'neuralmind/bert-base-portuguese-cased',\n",
    "    do_lower_case=True,\n",
    "    cache_dir=\"../models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [estou, conte, ##nte, com, a, compra, entrega,...\n",
       "1         [por, apenas, r, $, 1994, ., 20, ,, eu, conseg...\n",
       "2         [supera, em, ag, ##ilidade, e, pratic, ##idade...\n",
       "3         [meu, filho, amo, ##u, !, parece, de, verdade,...\n",
       "4         [a, entrega, foi, no, prazo, ,, as, americanas...\n",
       "                                ...                        \n",
       "132368    [vale, muito, ,, estou, usando, no, controle, ...\n",
       "132369    [pratic, ##o, e, bara, ##to, ,, super, indic, ...\n",
       "132370    [chegou, antes, do, prazo, previsto, e, corres...\n",
       "132371    [material, fraco, ,, poderia, ser, melhor, ., ...\n",
       "132372    [compre, ##i, esse, produto, ,, quando, chegou...\n",
       "Name: token_subword, Length: 129098, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[\"token_subword\"] = df_reviews[\"review_text\"].apply(lambda x: bert_tokenizer.tokenize(x))\n",
    "df_reviews[\"token_subword\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11259 tokens! Muito melhor né?\n",
    "<br> Percebam que abaixo já fizemos uma limpeza muito comum, que é de transformar o texto para letra minúscula, o que já homogeniza bastante os dados.\n",
    "<br>Percebam também que abaixo vários tokens puderam ser agrupados usando o método da subpalavra.\n",
    "<br>Exemplo: \"produto\" foi quebrado em um token, enquanto temos outro token para \"##s\". Dessa forma, mesmo uma palavra no plural ou no singular, conseguimos agrupá-la, tratando com um token o que representa a mesma coisa, e outro que contém a informação se é plural ou singular.\n",
    "<br>O mesmo acontece com vários outros tokens que conterão informações diferentes, como masculino e feminino, verbos e outros. Acima conseguimos ver um exemplo de divisão de verbo, onde um token foi dedicado a \"compre\" e outro para \"##i\", e um para \"amo\" e outro para \"##u\". As subpalavras sempre terão os ## no início, um padrão do WordPiece, que ajuda depois a voltarmos para a palavra original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 11259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 344998), ('##o', 179916), (',', 177398), ('e', 151051), ('o', 104337)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_counter = count_tokens(df=df_reviews, token_col=\"token_subword\")\n",
    "count_vocab[\"subword_counter\"] = len(subword_counter)\n",
    "subword_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pertencia', 1), ('##gaz', 1), ('hec', 1), ('ergue', 1), ('regressa', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_counter.most_common()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As subpalavras ajudam bastante porque conseguem manter informação ao mesmo tempo que agrupam de forma eficiente os tokens, reduzindo o vocabulário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Técnicas de normalização ou homogenização de texto\n",
    "Para fins didáticos, vamos seguir olhando a contagem de vocabulário por palavra para demonstrar a diferença das transformações aplicadas. Percebam que ao mesmo tempo que vamos limpando o texto, também é possível que a transformação faça com que percamos parte de informação, que pode ou não ser relevante dependendo do problema e a forma que formos tratar ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "from src.preprocessing import TextPreprocessing\n",
    "\n",
    "prep = TextPreprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformar o texto para letra minúscula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar o texto para minúsculo faz com que tokens sejam agrupados independente de teremos letras maiúscula ou minúscula. Essa é uma técnica muito utilizada e funciona bem para grande parte dos casos. <br> Você consegue imaginar quando é importante essa distinção? Podemos pensar em pessoas que usam as letras maiúsculas para expressar um sentimento ruim, quando estão \"gritando\", ou em problemas de reconhecimento de entidades nomeadas, organizações e nomes próprios geralmente começam com maiúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 59363\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text\"] = df_reviews[\"review_text\"].apply(lambda x: x.lower())\n",
    "df_reviews[\"cleaned_text_tokenized\"] = df_reviews[\"cleaned_text\"].apply(lambda x: word_tokenize(x))\n",
    "lower_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_tokenized\")\n",
    "count_vocab[\"lower_word_count\"] = len(lower_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos bastante nosso vocabulário, né? de 75k para 60k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover pontuação\n",
    "Remover pontuação também nos ajuda a reduzir o vocabulário. Que nem comentamos, as limpezas também fazem com que percamos informação, aqui podemos perder informação sobre separação de sentença, vários ! podem indicar um sentimento ruim, dependendo do problema e da modelagem escolhida, a pontuação pode ou não ser necessária.\n",
    "<br> Vamos continuar usando nossa coluna limpa para entendermos o impacto dessas limpezas no final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vemos que aumentamos nosso vocabulário ao invés de reduzirmos, o que não é o que queremos. Vamos entender porque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 66617\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text_tokenized_without_ponctuation\"] = df_reviews[\"cleaned_text\"].apply(lambda x: word_tokenize(re.sub(r\"[^\\w\\s]\", \"\", x)))\n",
    "lower_without_ponctuation_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_tokenized_without_ponctuation\")\n",
    "count_vocab[\"lower_without_ponctuation_word_count\"] = len(lower_without_ponctuation_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo podemos ver que tinhamos algumas palavras que eram separadas por causa de alguma pontuação. Quando retiramos a pontuação, as palavras se uniram, o que aumentou a variabilidade, aumentando a quantidade de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0207', '1490', 'bemsilencioso', 'defeitosvamos', 'dissoestou'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(itertools.islice((lower_without_ponctuation_word_count.keys() - lower_word_count.keys()), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se ao invés de retirar, substituíssemos a pontuação por um espaço? É assim que precisamos formular nosso raciocínio, criando hipótese, pensando em possibilidades de melhoria, e ir iterando para ver se estamos atingindo o resultado esperado. Agora nossa hipótese é: se eu substituir a pontuação por espaço, eu reduzo a quantidade de vocabulário.\n",
    "<br> Nossa hipótese foi confirmada, de 59.363 para 50.393 tokens. Estamos conseguindo agora nosso objetivo de reduzir a quantidade de vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 50393\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text\"] = df_reviews[\"cleaned_text\"].apply(lambda x: re.sub(r\"[^\\w\\s]\", \" \", x))\n",
    "df_reviews[\"cleaned_text_tokenized\"] = df_reviews[\"cleaned_text\"].apply(lambda x: word_tokenize(x))\n",
    "lower_without_ponctuation_space_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_tokenized\")\n",
    "count_vocab[\"lower_without_ponctuation_space_word_count\"] = len(lower_without_ponctuation_space_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padronizar para ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar aplicar o unidecode, que transformar todo o texto para um padrão chamado ASCII (American Standard Code for Information Interchange). Isso ajudará a tratar acentuação, aqueles caracteres especiais que vimos no início e mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 47715\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text\"] = df_reviews[\"cleaned_text\"].apply(lambda x: prep.preprocess_text(text=x, apply_unidecode=True))\n",
    "df_reviews[\"cleaned_text_tokenized\"] = df_reviews[\"cleaned_text\"].apply(lambda x: word_tokenize(x))\n",
    "unidecode_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_tokenized\")\n",
    "count_vocab[\"unidecode_word_count\"] = len(unidecode_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver se conseguimos limpar alguns dos casos de símbolos que vimos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otimo produto   indico para todos  muito show   kkkkkkkkkkk'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[df_reviews[\"review_text\"].str.contains('ķ')][\"cleaned_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gostei muito  sempre que estou em perigo tony vem me salvar com aquela linda musica do ac dc  recomendo muito este produto  aquela armadura faz horrores gente       ?    '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[df_reviews[\"review_text\"].str.contains('ʖ')][\"cleaned_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excelente! Ambos os símbolos foram tratados, aquele ķ virou um k normal, enquanto o ʖ foi transformado em ?. Além disso, já temos um vocabulário de 47.715 tokens, bem menor do que o que iniciamos de 75.717, né?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de Stopwords\n",
    "Quando lidamos principalmente com modelagems de algoritmos tradicionais que veremos nas próximas aulas, as palavras muito frequentes, chamadas de stopwords, como preposições, não ajudam na diferenciação de classes. Dessa forma, elas mais atrapalham a inferência na maioria das vezes do que ajudam, o que mais vai ajudar o algoritmo a diferenciar classes em uma classificação por exemplo são as palavras distintar mais específicas de uma determinada classe. \n",
    "<br>Podemos reduzir o vocabulário e melhorar a qualidade do modelo removendo essas palavras frequentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca nltk já possui uma lista de stopwords de diversos idiomas, como o português, abaixo vamos olhar quais são as palavras já mapeadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de stopwords mapeadas: 207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Quantidade de stopwords mapeadas: {len(prep.get_stopwords())}\")\n",
    "prep.get_stopwords()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar essa biblioteca como ponto de partida e ir adicionando ou removendo palavras quando acharmos relevante para nosso problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 47563\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text\"] = df_reviews[\"cleaned_text\"].apply(lambda x: prep.preprocess_text(text=x, remove_stopwords=True))\n",
    "df_reviews[\"cleaned_text_tokenized\"] = df_reviews[\"cleaned_text\"].apply(lambda x: word_tokenize(x))\n",
    "stopword_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_tokenized\")\n",
    "count_vocab[\"stopword_word_count\"] = len(stopword_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduzimos pouco o vocabulário, até porque só temos 207 stopwords, mas isso, dependendo do problema em questão, pode fazer a diferença! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 25620\n"
     ]
    }
   ],
   "source": [
    "df_reviews[\"cleaned_text_stemming\"] = df_reviews[\"cleaned_text\"].apply(lambda x: prep.preprocess_text(text=x, apply_stemming=True))\n",
    "df_reviews[\"cleaned_text_stemming_tokenized\"] = df_reviews[\"cleaned_text_stemming\"].apply(lambda x: word_tokenize(x))\n",
    "stemming_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_stemming_tokenized\")\n",
    "count_vocab[\"stemming_word_count\"] = len(stemming_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming é o processo de reduzir as palavras às suas raízes ou radicais. Percebam abaixo que esse método pode resultar em palavras não reconhecíveis, removendo sufixos e prefixos. Dessa forma, content foi reduzido para cont, compra foi reduzido para compr e assim por diante. Esse método consegue reduzir bastante o vocabulário, veja que chegamos a 25.620, 33% do vocabulário que iniciamos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>cleaned_text_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>cont compr entreg rap unic problem americ troc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>apen r 1994 20 consegu compr lind cop acril</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>sup agil pratic outr panel eletr costum us out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>filh amou parec verdad tant detalh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>entreg praz americ esta parab smart tv boa nav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...   \n",
       "4  A entrega foi no prazo, as americanas estão de...   \n",
       "\n",
       "                               cleaned_text_stemming  \n",
       "0  cont compr entreg rap unic problem americ troc...  \n",
       "1        apen r 1994 20 consegu compr lind cop acril  \n",
       "2  sup agil pratic outr panel eletr costum us out...  \n",
       "3                 filh amou parec verdad tant detalh  \n",
       "4  entreg praz americ esta parab smart tv boa nav...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[[\"review_text\", \"cleaned_text_stemming\"]].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatizing\n",
    "Reduz as palavras à sua forma canônica ou dicionária, o que geralmente produz resultados mais precisos e legíveis do que apenas cortar sufixos e prefixos como no stemming, além de manter sua integridade semântica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário total: 41976\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = spacy.load('pt_core_news_sm')\n",
    "\n",
    "df_reviews[\"cleaned_text_lemmatizing\"] = df_reviews[\"cleaned_text\"].apply(lambda x: prep.preprocess_text(text=x, apply_lemmitization=True, lemmatizer=lemmatizer))\n",
    "df_reviews[\"cleaned_text_lemmatizing_tokenized\"] = df_reviews[\"cleaned_text_lemmatizing\"].apply(lambda x: word_tokenize(x))\n",
    "lemmatizing_word_count = count_tokens(df=df_reviews, token_col=\"cleaned_text_lemmatizing_tokenized\")\n",
    "count_vocab[\"lemmatizing_word_count\"] = len(lemmatizing_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo podemos ver que os verbos foram normalizados, compra virou comprar, consegui virou conseguir e assim por diante. Além disso, outras foi normalizado para outro, tratamento de plurais acontecem na lemmatização, reduzindo nosso vocabulário. No entanto, podemos ver que não reduzimos tanto quanto usando o método stemming, veja que chegamos a 41.976 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>cleaned_text_lemmatizing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>contente compra entregar raper unico problema ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>apenas r 1994 20 conseguir comprar lir copo ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>superar agilidade praticidade outro panela ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>filho amar parecer verdade tanto detalhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>entregar prazo americana estao parabem smart t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...   \n",
       "4  A entrega foi no prazo, as americanas estão de...   \n",
       "\n",
       "                            cleaned_text_lemmatizing  \n",
       "0  contente compra entregar raper unico problema ...  \n",
       "1  apenas r 1994 20 conseguir comprar lir copo ac...  \n",
       "2  superar agilidade praticidade outro panela ele...  \n",
       "3           filho amar parecer verdade tanto detalhe  \n",
       "4  entregar prazo americana estao parabem smart t...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[[\"review_text\", \"cleaned_text_lemmatizing\"]].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo podemos ver nossa evolução a medida que fomos limpando nosso texto, saindo de 75.717 e chegando ao mínimo de 25.620. Interessante notar que a tokenzização por subpalavra teve um vocabulário ainda menor (11.259) do que com todas essas limpezas, e perde muito menos informação. Interessante, não? Por isso esses métodos de tokenização são bastante usados, principalmente em algoritmos complexos como LLMs, e considerados o estado da arte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caracter_counter': 212,\n",
       " 'word_counter': 75717,\n",
       " 'subword_counter': 11259,\n",
       " 'lower_word_count': 59363,\n",
       " 'lower_without_ponctuation_word_count': 66617,\n",
       " 'lower_without_ponctuation_space_word_count': 50393,\n",
       " 'unidecode_word_count': 47715,\n",
       " 'stopword_word_count': 47563,\n",
       " 'stemming_word_count': 25620,\n",
       " 'lemmatizing_word_count': 41976}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo com todos as limpezas que fizemos, ainda temos um volume de vocabulário bem alto, podemos:\n",
    "- Continuar com as limpezas, tratando urls, removendo números, removendo tokens com menos de 2 caracteres ou outras formas, dependendo do contexto e do problema;\n",
    "- Definir um vocabulário máximo, removendo ou substituindo os tokens menos frequentes por \"desconhecido\" até chegarmos no tamanho de vocabulário definido;\n",
    "- Definir um vocabulário, substituir os tokens menos frequentes pelos tokens mais frequentes usando esse vocabulário e métodos de similiridade de texto;\n",
    "<br>Entre outros possíveis tratamentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similaridade de texto (Correspondência Fuzzy)\n",
    "Para corrigir erros de digitação, variações ortográficas, abreviações ou outros tipos de variações, podemos usar técnicas de correspondência fuzzy. Ela permite encontrar correspondências aproximadas entre textos, levando em consideração a semelhança entre eles, mesmo que não sejam idênticos. \n",
    "<br> Alguns exemplos de técnicas para correspondência fuzzy são:\n",
    "- <b>Similaridade de Jaro:</b> Valor de 0 a 1, onde 1 é correspondência perfeita e 0 nenhuma correspondência. Basicamente é uma soma de correlações entre caracteres dos dois textos, ponderado pelo tamanho deles. \n",
    "\n",
    "$d_j=\\frac{m}{3a}+\\frac{m}{3b}+\\frac{m-t}{3m}$\n",
    "\n",
    "<br>onde:\n",
    "- m é o número de correlações entre caracteres;\n",
    "- a e b são os tamanhos dos textos a serem comparados;\n",
    "- t é o número de transposições."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <b>Similaridade de Jaro-Winkler:</b> Estende a Similaridade de Jaro adicionando uma constante que favorece quando a comparação ocorre com textos que possuem equivalência longa no início. </b>\n",
    "- <b>Distância de Levenshtein:</b> Conta a quantidade mínima de operações que são necessárias para transformar um texto em outro. </b>\n",
    "- <b>Distância de Damerau-Levenshtein:</b> Estende a distância de Levenshtein ao adicionar a transposição adjacente entre as possíveis operações. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver na prática como funciona? Vamos começar delimitando nosso máximo de vocabulário para 95% do que temos até a limpeza da removação das stopwords que foi a última limpeza antes do stemming e do lemmatizing, visto que elas alteram/cortam as palavras. Aqui 95% foi um número arbitrário, de acordo com o problema em questão, podemos aumentar ou reduzir. Dessa forma, estamos dizendo que 95% dos tokens são válidos, e os outros 5% vamos tentar tratá-los.\n",
    "<br> Vamos começar com um exemplo de correção ortográfica que vamos retirar do nosso vocabulário de tratamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47563"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = int(len(stopword_word_count) * 0.95)\n",
    "items = stopword_word_count.most_common()\n",
    "vocab_valido = dict(items[:max_vocab])\n",
    "vocab_tratamento = dict(items[max_vocab:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45184, 2379)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_valido), len(vocab_tratamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'revigoram': 1,\n",
       " 'stevia': 1,\n",
       " 'porqua': 1,\n",
       " 'schulz': 1,\n",
       " 'regressao': 1,\n",
       " 'ny': 1,\n",
       " 'largam': 1,\n",
       " '21kilis': 1,\n",
       " '1metro': 1,\n",
       " 'fortinha': 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(items[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos! `porqua` deveria ser `porque`. Vamos ver se conseguimos tratá-lo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aonde eu encontro carvão de motor desta maquina, porqua a da schulz eu não estou encontrando.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews[df_reviews[\"cleaned_text\"].str.contains(\"porqua\")][\"review_text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vamos testar os métodos para tentar corrigir a palavra `porqua` para `porque`. Percebam que os métodos de Jaro são quanto maior mais similares são os textos, enquanto os métodos com levenshtein são quanto menor melhor, visto que precisamos de menos operações para chegar de um texto ao outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jaro_similarity': {'most_likely_token': 'porqu',\n",
       "  'score': 0.9444444444444445},\n",
       " 'jaro_winkler_similarity': {'most_likely_token': 'porqu',\n",
       "  'score': 0.9666666666666667},\n",
       " 'damerau_levenshtein_distance': {'most_likely_token': 'porque', 'score': 1},\n",
       " 'levenshtein_distance': {'most_likely_token': 'porque', 'score': 1}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jellyfish\n",
    "import operator\n",
    "\n",
    "similaridades = [\n",
    " \"jaro_similarity\",\n",
    " \"jaro_winkler_similarity\",\n",
    " \"damerau_levenshtein_distance\",\n",
    " \"levenshtein_distance\"\n",
    "]\n",
    "\n",
    "texto = \"porqua\"\n",
    "resultados = {}\n",
    "for distancia in similaridades:\n",
    "  score = {\n",
    "      valid_token: getattr(jellyfish, distancia)(\n",
    "        texto, valid_token\n",
    "      )\n",
    "      for valid_token in vocab_valido\n",
    "  }\n",
    "  score = max(score.items(), key=operator.itemgetter(1)) if \"jaro\" in distancia else min(score.items(), key=operator.itemgetter(1))\n",
    "  resultados[distancia] = {\"most_likely_token\": score[0] ,\"score\": score[1]}\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os métodos de levenshtein conseguiram identificar a palavra correta! Enquanto os métodos de jaro acabaram achando `porqua` mais similar com `porqu`. Outro ponto interessante é que jaro_winkler_similarity deu uma pontuação maior do que somente jaro_similarity, visto que a maior parte da similaridade dos dois textos está no início `porqu`. Os métodos Damerau Levenshtein e Levenshtein deram o mesmo resultado, visto que não foi necessário nenhuma transposição para chegar de `porqua` para `porque`, somente a alteração de uma palavra, o `a` pelo `e`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usando a similaridade para limitar o vocabulário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar a biblioteca rapidfuzz que já implementa a comparação de um texto com uma lista, extraindo o mais similar de acordo com a distância de Levenshtein e retornando a pontuação normalizada também. Com isso, podemos usá-la para limitar nosso vocabulário, alterando os textos com alta similaridade, enquanto os demais irão para um token chamado \"desconhecido\". Poderíamos ter implementado um loop usando a biblioteca jellyfish também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O score de 80 foi um número arbitrário, podemos aumentar ou reduzir de acordo com o problema em questão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituindo token estatisticos pelo token estatisticas\n",
      "Substituindo token sigma pelo token siga\n",
      "Substituindo token destravou pelo token desconhecido\n",
      "Substituindo token steelcase pelo token desconhecido\n",
      "Substituindo token jim pelo token desconhecido\n",
      "Substituindo token andadores pelo token animadores\n",
      "Substituindo token arrendondada pelo token arredondada\n",
      "Substituindo token guste pelo token goste\n",
      "Substituindo token calida pelo token calidad\n",
      "Substituindo token actividades pelo token atividades\n",
      "Substituindo token estudiar pelo token estudar\n",
      "Substituindo token a16 pelo token desconhecido\n",
      "Substituindo token derrotara pelo token derrotar\n",
      "Substituindo token xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx pelo token xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Substituindo token buddemeryer pelo token buddemeyer\n",
      "Substituindo token interestellar pelo token desconhecido\n",
      "Substituindo token buzztech pelo token desconhecido\n",
      "Substituindo token hypetech pelo token desconhecido\n",
      "Substituindo token 452 pelo token desconhecido\n",
      "Substituindo token buchos pelo token buchas\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "\n",
    "items = list(vocab_tratamento.items())\n",
    "itens_to_treat = dict(items[:20])\n",
    "\n",
    "for key, value in itens_to_treat.items():\n",
    "    new_token, score, _ = process.extractOne(key, vocab_valido.keys(), scorer=Levenshtein.normalized_similarity)\n",
    "    replacement = new_token if score >= 0.80 else \"desconhecido\"\n",
    "    df_reviews[\"cleaned_text_fuzzy\"] = df_reviews[\"cleaned_text\"].str.replace(f\" {key} \", f\" {replacement} \")\n",
    "    print(f\"Substituindo token {key} pelo token {replacement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que grande parte das substituições fizeram bastante sentido, limpando os tokens para não ter números e letras teríamos um resultado ainda melhor. <br>\n",
    "<br> Agora você já sabe como limpar os textos! Na próxima aula veremos como transformar os tokens em representações numéricas que irão alimentar nosso modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
