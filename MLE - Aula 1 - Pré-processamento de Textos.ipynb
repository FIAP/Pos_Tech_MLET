{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8efa82-08c5-4823-8b5b-7020b75b4c48",
   "metadata": {},
   "source": [
    "# Introdução e Técnicas de Processamento de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd545293-6092-4d32-be23-731959e32b8f",
   "metadata": {},
   "source": [
    "Na aula de hoje, veremos o que é Processamento de Linguagem Natural, alguns exemplos de aplicações e quais as principais técnicas de pré-processamento de texto. Durante a aula, mesclaremos conteúdo teórico e prático usando Jupyter Notebook a fim de entendermos e praticarmos sobre o conteúdo proposto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2634358-fd66-4252-8dcc-1cc844d6fb79",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67833a98-d40b-4e23-81d9-726b3747a015",
   "metadata": {},
   "source": [
    "O campo de processamento de linguagem natural (NLP) passou por uma mudança dramática nos últimos anos, tanto em termos de metodologia quanto em termos de aplicativos suportados. Os avanços metodológicos têm variado desde novas formas de representar documentos a novas técnicas de síntese de linguagem. Com eles, surgiram novos aplicativos que vão desde sistemas de conversação abertos até técnicas que usam linguagem natural para a interpretabilidade do modelo. Por fim, esses avanços permitiram que a NLP ganhasse espaço em áreas relacionadas, como visão computacional e sistemas de recomendação. Este último será objeto de estudo nosso futuramente.\n",
    "\n",
    "Num sentido amplo, Processamento de Linguagem Natural (NLP) trata de qualquer tipo de manipulação computacional de linguagem natural, desde uma simples contagem de frequências de palavras para comparar diferentes estilos de escrita, até o “entendimento” completo de interações humanas (pelo menos no sentido de oferecer uma resposta útil a eles).\n",
    "\n",
    "As tecnologias baseadas em NLP estão se tornando cada vez mais pervasivas e, diante das interfaces homem-máquina mais naturais e meios mais sofisticados de armazenamento de informações, o processamento de linguagem tem alcançado um papel central numa sociedade da informação multilíngue.\n",
    "\n",
    "Por conta disso, NLP está rapidamente se tornando uma habilidade necessária exigida por engenheiros, gerentes de produto, cientistas, estudantes e entusiastas que desejam construir aplicativos com base em dados de linguagem natural. Por um lado, novas ferramentas e bibliotecas, para NLP e aprendizado de máquina tornaram a modelagem de linguagem natural mais acessível do que nunca. Mas, por outro lado, os recursos para aprender NLP devem visar esse público diversificado e sempre crescente.\n",
    "\n",
    "Como dito, NLP permite interação com sistemas computacionais em linguagem humana. Entretanto, computadores entendem apenas dados binários, por exemplo, 0 e 1. \n",
    "\n",
    "Para exemplificar o quão importante NLP se tornou em nossa vida, aqui vão algumas aplicações:\n",
    "\n",
    "1. Plataformas de e-mail usam NLP para classificar mensagens (spam ou legítimas), priorização na caixa de entrada e auto-complete;\n",
    "2. Assistentes baseados em voz, tais como Amazon Alexa, Apple Siri, Google Assistant ou Microsoft Cortana são baseados em técnicas de NLP para interagir com os usuários, entende-los e responde-los corretamente;\n",
    "3. Plataformas de busca (Search engine), como Google ou Bing, usam NLP para entendimento de query (informação que o usuário digitou), recuperação da informação e ranqueamento, para citar alguns;\n",
    "4. Tradução de máquina, como o Google Translate, é construído em cima de técnicas de NLP\n",
    "5. Além disso, NLP pode ser usado em uma variedade de campos, como jurídico, saúde, varejo, atendimento, marketing e outros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb5689-11c2-4137-9466-ef6169fe3f49",
   "metadata": {},
   "source": [
    "Mas modelar problemas de NLP não é uma tarefa trivial, pelo contrário, é bem desafiador. Vamos ver dois exemplos. O primeiro é usado em sistemas de busca:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02524a3-c21b-4409-9e56-232f21867496",
   "metadata": {},
   "source": [
    "<img src=\"img/beagles.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1b314-de4d-4091-8e41-54ea0c8af472",
   "metadata": {},
   "source": [
    "Outro caso bastante comum é a ambiguidade:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041215ce-6ee5-403f-8707-5412f6440b27",
   "metadata": {},
   "source": [
    "<img src=\"img/ambiguidade.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966f796-ddc9-4134-9364-4266cd54f20a",
   "metadata": {},
   "source": [
    "Qual frase possui a interpretação correta? Como você justificaria sua escolha?\n",
    "\n",
    "Perceba que a tarefa é mais complexa do que imaginamos. E criar um sistema de NLP que consiga interpretar tais textos e fornecer uma resposta coerente é realmente uma tarefa árdua. No entanto, precisamos começar pelo começo. Na sessão a seguir, discutiremos quais as principais técnicas de pré-processamento que usamos para preparar o texto para que possam ser usados em sistemas de inteligência Artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240de05-7503-4e15-8eac-7c0e88713e6d",
   "metadata": {},
   "source": [
    "## Pipeline de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58357f-3903-4eb7-aa8f-d3e6260ccd51",
   "metadata": {},
   "source": [
    "Quando falamos de NLP, geralmente usamos técnicas de Machine Learning. Elas são aplicadas a dados textuais da mesma maneira que são usadas em outros tipos de dados, como imagens ou dados estruturados. \n",
    "\n",
    "Toda abordagem de Machine Learning para NLP, seja ela supervisionada ou não supervisionada, pode ser descrita em três passos comuns:\n",
    "\n",
    "1. Extrair features de um texto.\n",
    "2. Usar uma representação dessas features para aprender um modelo.\n",
    "3. Avaliar e melhorar o modelo.\n",
    "\n",
    "De maneira mais geral, precisamos de mais algumas etapas. O diagrama abaixo apresenta um pipeline genérico para NLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df06b3-aaba-45d9-a055-f06a036c0e16",
   "metadata": {},
   "source": [
    "<img src=\"img/pipeline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd32f3f-0a27-4894-b380-1bbad398c29d",
   "metadata": {},
   "source": [
    "O primeiro passo é o de “aquisição de dados”, que consiste em obter os dados textuais que serão usados para treinar nosso modelo de Machine Learning. Geralmente, os dados estão disponíveis em bancos de dados nas próprias empresas. Às vezes, você encontra algumas bases públicas, em outros casos, você precisa ir capturando as informações aos poucos. Essa parte é importante, apesar de não ser o foco da disciplina, pois quanto mais dados, melhor será o nosso modelo. \n",
    "\n",
    "No segundo passo, iremos fazer a limpeza desse texto, removendo qualquer coisa que não seja texto, tais como metadados, links, entre outros. O terceiro passo é o pré-processamento. Às vezes, ele é executado junto com a limpeza dos dados, consistindo numa série de técnicas que serão objeto de estudo dessa aula. \n",
    "\n",
    "O próximo passo, também conhecido como “feature extraction”, consiste em extrair do texto quais características melhor o descrevem e imputá-las em algoritmos de machine learning. A parte de modelagem consiste em escolher uma técnica de machine learning e usar os dados tratados para resolver um problema específico. Logo em seguida, devemos avaliar esse modelo, a partir de alguns critérios que apresentaremos no decorrer da disciplina. \n",
    "\n",
    "Por fim, faremos o deploy do nosso modelo, disponibilizando o modelo treinado em ambiente produtivo e, frequentemente, monitorando desvios, erros e realizando correções, quando necessário, treinando o modelo novamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6bc0b-9244-41de-89d3-1a23572f7234",
   "metadata": {},
   "source": [
    "### Técnicas de Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1794f5-f2b2-4c7c-bf03-cc164bee0024",
   "metadata": {},
   "source": [
    "Como dito anteriormente, o primeiro passo a ser feito em NLP é extrair features de um texto. Mais precisamente, precisamos adquirir os dados, limpá-los, fazer o pré-processamento e extrair as características, transformando os dados para que possam ser imputados em modelos de machine learning. Vamos tratar desses passos nessa sessão. Importante dizer que, a título de didática, consideramos que já temos os dados disponíveis. \n",
    "\n",
    "Como sabemos, os computadores lidam com números. Qualquer outro tipo de dado (como áudio, texto, imagens, etc.) precisa passar por um processo de transformação para números. \n",
    "\n",
    "Para realizar essa transformação, antes precisamos fazer o pré-processamento de dados, cujo objetivo é preparar os textos para criar um espaço de características adequado. \n",
    "\n",
    "O primeiro passo é a tokenização dos dados. A tokenização nada mais é que uma sequência de “n” elementos de uma sequência maior, denominadas n-gramas. Seu objetivo é transformar os textos em números. Os tipos de n-grama são definidos pela quantidade de elementos que os compõem. Unigramas (N = 1), Bigramas (N = 2) e Trigramas (N = 3).\n",
    "\n",
    "A ideia é dividir o texto, geralmente usando “espaço” como separador, em tokens, para realizar outras atividades.\n",
    "Considere as seguintes frases como exemplos: \n",
    "\n",
    "* Eu gosto de assistir jogos de futebol.\n",
    "* Já eu, prefiro assistir jogos de basquete.\n",
    "\n",
    "\n",
    "Vamos entender como construir uma representação usando unigrama a partir do código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b2161e-dad3-46b0-a3c4-7c78314501aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu gosto de assistir jogos de futebol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já eu, prefiro assistir jogos de basquete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text\n",
       "0      Eu gosto de assistir jogos de futebol\n",
       "1  Já eu, prefiro assistir jogos de basquete"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "      'Eu gosto de assistir jogos de futebol',\n",
    "      'Já eu, prefiro assistir jogos de basquete'\n",
    "    ]\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4b1ea6-54c0-4851-b0ed-1eba6e02f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1\n",
      "assistir  1  1\n",
      "basquete  0  1\n",
      "de        2  1\n",
      "eu        1  1\n",
      "futebol   1  0\n",
      "gosto     1  0\n",
      "jogos     1  1\n",
      "já        0  1\n",
      "prefiro   0  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1f0b4-07aa-4a27-88f5-93bdcaac836a",
   "metadata": {},
   "source": [
    "Podemos criar bigramas e trigramas apenas alterando os valores do parâmetro `ngram_range`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371e00f4-503f-4d5b-b747-33aba1d482e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0  1\n",
      "assistir jogos    1  1\n",
      "de assistir       1  0\n",
      "de basquete       0  1\n",
      "de futebol        1  0\n",
      "eu gosto          1  0\n",
      "eu prefiro        0  1\n",
      "gosto de          1  0\n",
      "jogos de          1  1\n",
      "já eu             0  1\n",
      "prefiro assistir  0  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e4efe5-bd2d-4d9d-b7a3-f10d3cefee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0  1\n",
      "assistir jogos de       1  1\n",
      "de assistir jogos       1  0\n",
      "eu gosto de             1  0\n",
      "eu prefiro assistir     0  1\n",
      "gosto de assistir       1  0\n",
      "jogos de basquete       0  1\n",
      "jogos de futebol        1  0\n",
      "já eu prefiro           0  1\n",
      "prefiro assistir jogos  0  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726ffcb-dc83-4acf-a2f6-33de705979b8",
   "metadata": {},
   "source": [
    "Para realizar a tokenização, podemos implementar uma solução própria ou usar a biblioteca NLTK. NLTK, ou Natural Language Toolkit, é uma plataforma para construir programas em Python para trabalhar com dados de linguagem humana. Vamos usá-la para realizar a maioria das tarefas de pré-processamento que compõe o pipeline de transformação de dados textuais. Observe o exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defafc7a-3a80-405d-a7cb-68d49a263dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'futebol',\n",
       " 'brasileiro',\n",
       " 'é',\n",
       " 'o',\n",
       " 'melhor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " '.',\n",
       " 'Você',\n",
       " 'concorda',\n",
       " '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "exemplo = 'O futebol brasileiro é o melhor do mundo. Você concorda?'\n",
    "words = word_tokenize(exemplo)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d9de4-ee54-4266-8938-222a40f2bd73",
   "metadata": {},
   "source": [
    "Outra técnica que auxilia muito no pré-processamento é o Regex. “Expressão regular” é uma maneira de identificar padrões em sequências de caracteres. No Python, o módulo re provê um analisador sintático, que permite o uso de tais expressões. Os padrões definidos através desses caracteres têm significado especial para o analisador.\n",
    "\n",
    "Essa solução é muito usada como um complemento para soluções mais complexas de NLP, principalmente quando é necessário capturar informações de padrões bem definidos e estabelecidos, como CPF, RG, e-mail, entre outros. \n",
    "\n",
    "A imagem abaixo resume os principais caracteres que são usados para definir um padrão a ser procurado numa string:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc49b0e-136c-4d5b-8dbb-88eb83d904c0",
   "metadata": {},
   "source": [
    "<img src=\"img/regex.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa2186-557c-4ce9-8386-9b3be643e07a",
   "metadata": {},
   "source": [
    "Observe o exemplo para entendermos o uso do Regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4876135e-638e-487e-b308-a1f8d5d39494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen, Aerosmith & Beatles -> ['Queen', 'Aerosmith', 'Beatles']\n",
      "Phone Num :  2004-959-559 \n",
      "Phone Num :  2004959559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "rex = re.compile('\\w+') \n",
    "bandas = 'Queen, Aerosmith & Beatles'\n",
    "print (bandas, '->', rex.findall(bandas))\n",
    "phone = \"2004-959-559 # This is Phone Number\"\n",
    "num = re.sub('#.*$', \"\", phone) \n",
    "print (\"Phone Num : \", num)\n",
    "num = re.sub(r'\\D', \"\", phone)\n",
    "print (\"Phone Num : \", num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fd805-0f34-432b-81e7-ac6ff049a1be",
   "metadata": {},
   "source": [
    "Adicionalmente à tokenização e ao regex, temos as *stop-words*. Quando lemos um texto, percebemos que algumas palavras sempre aparecem, mas elas não contribuem para uma interpretação mais sólida de uma frase. Tais palavras são chamadas “stop-words”. Normalmente, são artigos, advérbios, preposições, conectivos e até alguns verbos. \n",
    "\n",
    "Geralmente, quando nos deparamos com um texto, optamos por remover tais palavras, fazendo com que nosso modelo se concentre no que, de fato, é importante dentro de uma frase. Tecnicamente falando, estamos reduzindo o espaço de características. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393e756e-e35f-4bc5-ad0c-ba25941df795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43b229-4eb6-4b50-81ed-262889e490af",
   "metadata": {},
   "source": [
    "Geralmente, quando nos deparamos com um texto, optamos por remover tais palavras, fazendo com que nosso modelo se concentre no que, de fato, é importante dentro de uma frase. Tecnicamente falando, estamos reduzindo o espaço de características.\n",
    "\n",
    "Por falar nele, outra técnica que auxilia o modelo é a normalização, que consiste em fazer com que as palavras tenham a mesma representação morfológica, a fim de não gerar vieses. Por exemplo, caso eu tenha a palavra “Que” e, posteriormente, a palavra “que”, elas não podem ser tratadas como distintas. Para resolver isso, colocamos todas as palavras em letra minúscula. \n",
    "\n",
    "Outro fator de normalização é o tratamento que damos aos tempos verbais das palavras. Num sentido simples, “foi” e “será” têm origem na mesma palavra: “é”. Logo, em alguns casos, fazemos com que as palavras sejam reescritas em sua forma raiz. Esse processo é conhecido como lematização. Uma técnica mais simples, mas similar, é a stemização, que consiste em podar (ou, pelo menos, tentar podar) a palavra até um radical mais próximo. \n",
    "\n",
    "Para entendermos o funcionamento dessas três técnicas, vamos analisar as seguintes frases:\n",
    "\n",
    "* O carro que estava quebrado voltou a funcionar.\n",
    "* Meu carro quebrou e não está funcionando.\n",
    "\n",
    "O primeiro passo é realizar a vetorização dos dados, ou seja, transformar palavras em números. Nesse caso, é colocado “0” para quando a palavra não está na frase e “1” para quando está. O conjunto de palavras distintas forma o que chamamos de dicionário. O resultado obtido é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4c25fb-4314-4cdc-bd15-6a9d3e4703ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'carro', 'que', 'estava', 'quebrado', 'voltou', 'a', 'funcionar']\n",
      "['Meu', 'carro', 'quebrou', 'e', 'não', 'está', 'funcionando']\n"
     ]
    }
   ],
   "source": [
    "ex1 = 'O carro que estava quebrado voltou a funcionar'\n",
    "ex2 = 'Meu carro quebrou e não está funcionando'\n",
    "\n",
    "print(word_tokenize(ex1))\n",
    "print(word_tokenize(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f63479e-09f7-4488-bf89-568f6e869b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "estava       1  0\n",
      "está         0  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "meu          0  1\n",
      "não          0  1\n",
      "que          1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':[ex1,ex2]})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df330272-4845-4263-9682-3825f3c32603",
   "metadata": {},
   "source": [
    "O próximo passo é remover as *stop-words*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb2d8890-bdda-4aff-8ecb-920398068d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550698e5-105a-41a7-9b5d-362186ea928e",
   "metadata": {},
   "source": [
    "Entretanto, ainda temos variações de uma mesma palavra dentro do meu conjunto de características:\n",
    "    \n",
    "* Funcionar: funcionar, funcionando.\n",
    "* Quebrar: quebrado, quebrou.\n",
    "\n",
    "Podemos melhorar a representação de texto usando stemming e lemmatization, para transformar palavras/tokens num formato padrão. O que as diferencia é a maneira com que elas atingem esse objetivo. A decisão de qual usar depende do trabalho e, muitas vezes, isso é decidido baseado em testes. Vamos entender os dois casos antes de decidir qual usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ae9e762-55c9-4f7d-8629-72d7c582322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "examples = [\n",
    "   \"go\",\"going\",\n",
    "   \"goes\",\"gone\",\"went\"\n",
    "]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in examples:\n",
    "  print(wnl.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b437da55-63a7-4235-8ec1-bee134e9d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\n",
    "    \"connection\",\"connections\",\n",
    "    \"connective\",\"connecting\",\"connected\"\n",
    "]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b46f1-52cd-4341-8d5e-23d82dc1646b",
   "metadata": {},
   "source": [
    "Entretanto, para o português os resultados não são bons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916984bf-fbca-47c1-9b8e-23424b8d5a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conecta\n",
      "conectado\n",
      "conectamo\n",
      "desconectado\n",
      "conectividad\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fdcb1-18a1-4a17-9d51-41b1c9eb2369",
   "metadata": {},
   "source": [
    "Uma alternativa é usar o RSLPStemmer, que produz melhores resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c150da-8750-477f-b63e-cdd67c7fd801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conect\n",
      "conect\n",
      "conect\n",
      "desconect\n",
      "conect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(rslp.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31978e54-b580-4b46-840f-1dfe85569976",
   "metadata": {},
   "source": [
    "Assim, ele será nossa escolha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe7d15c-ed48-49f4-8b6e-4d0f9478f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1 = \" \".join([rslp.stem(x) for x in word_tokenize(ex1)])\n",
    "stem2 = \" \".join([rslp.stem(x) for x in word_tokenize(ex2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318d0cad-c0ed-4093-a489-3a3a80cf5651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  1\n",
      "carr     1  1\n",
      "est      1  1\n",
      "funcion  1  1\n",
      "quebr    1  1\n",
      "volt     1  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':[stem1,stem2]})\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c7c73-50eb-4033-8ea3-ecacec50023e",
   "metadata": {},
   "source": [
    "Ou seja, depois de aplicar todo esse processo de normalização, reduzi meu espaço de característica, o que contribui para que meu modelo se concentre apenas no essencial e tenha maiores chances de produzir um ótimo resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830cc64-66ef-47fb-8dce-51255c7a31cf",
   "metadata": {},
   "source": [
    "## POS-Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cace98-addd-44cb-bfcd-5c30268fd058",
   "metadata": {},
   "source": [
    "Na escola primária, aprendemos a diferença entre substantivo, verbos, advérbios e adjetivos. Tais classes gramaticais são categorias úteis para muitas tarefas de processamento de linguagem natural. POS-Tagging é uma ferramenta usada no processamento de linguagem natural (NLP), que permite que os algoritmos entendam a estrutura gramatical de uma frase e desambiguam palavras que têm vários significados.\n",
    "\n",
    "Aqui, teremos os seguintes objetivos:\n",
    "\n",
    "* Quais são as categorias léxicas (classes gramaticais) e como elas são usadas em NLP.\n",
    "* Uma boa estrutura de dados em Python, para armazenar palavras e suas categorias.\n",
    "* Como podemos marcar (taguear) automaticamente cada palavra de um texto com sua classe.\n",
    "\n",
    "Observe a imagem a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7aba3-aa9b-4c96-8f9a-305008079ce2",
   "metadata": {},
   "source": [
    "<img src=\"img/tagger.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36151043-6479-4298-87a0-8cbbae2fa49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d332555-afa1-4216-8915-609114f7fc31",
   "metadata": {},
   "source": [
    "Mas, afinal, qual a real utilidade disso? Considere as seguintes palavras: Woman (substantivo), bought (verbo), over (preposição) e The (determinante). \n",
    "\n",
    "Com o POS-tagging, é possível encontrar palavras que pertençam à mesma classe gramatical. Vamos usar o corpus Brown (esse [link](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html) fornece mais informações sobre o corpus) como exemplo para entendermos esse processo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfd1d95b-bac3-4c65-be6c-64b948813685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33349517-a8a5-4810-9790-4b753b7173b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf3acbf4-d2f0-4322-b9b7-8b78071aad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86f3e74d-4a58-4812-9c25-3b66c563d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n"
     ]
    }
   ],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848e2f8-23aa-414e-a06c-ea2099e56652",
   "metadata": {},
   "source": [
    "Com isso, conseguimos treinar um tagger para taguear palavras novas. Entretanto, NLTK não possui suporte nativo ao português, mas é possível fazer o download de um Corpus para resolver nosso problema. \n",
    "\n",
    "Aqui, vamos usar o Corpus Floresta. O projeto Floresta Sintá(c)tica é uma colaboração entre a Linguateca e o projecto VISL. Contém textos em português (do Brasil e de Portugal) anotados (analisados) automaticamente pelo analisador sintático PALAVRAS e revistos por linguistas. Mais informações podem ser obtidas nesse [link](https://www.linguateca.pt/Floresta/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d53da21-616a-485a-a53c-a7f284123a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Um', '>N+art'), ('revivalismo', 'H+n'), ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "floresta.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc26de5-69d8-48a4-b7d2-1fceb3556bb3",
   "metadata": {},
   "source": [
    "Veremos que a tag atribuída a uma palavra dependerá da própria palavra e seu contexto numa sentença. Assim, o tagueamento ocorre a nível de sentença, não de palavra. \n",
    "\n",
    "Vamos criar uma função para simplificar o nome de uma tag. As informações sobre a abreviação de cada tag podem ser consultadas nesse [link](https://www.linguateca.pt/Floresta/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e707d8-daac-42a6-8488-45c8f3c5046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('um', 'art'),\n",
       " ('revivalismo', 'n'),\n",
       " ('refrescante', 'adj'),\n",
       " ('o', 'art'),\n",
       " ('7_e_meio', 'prop'),\n",
       " ('é', 'v-fin'),\n",
       " ('um', 'art'),\n",
       " ('ex-libris', 'n'),\n",
       " ('de', 'prp'),\n",
       " ('a', 'art')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplify_tag(t):\n",
    "  if \"+\" in t:\n",
    "    return t.split(\"+\")[1]\n",
    "  return t \n",
    "\n",
    "twords = nltk.corpus.floresta.tagged_words()\n",
    "twords = [(w.lower(),simplify_tag(t)) for (w,t) in twords]\n",
    "twords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc20c2-8a91-4d24-8f1b-c944b76f55f5",
   "metadata": {},
   "source": [
    "Vamos analisar as seguintes estratégias de tagueamento:\n",
    "    \n",
    "* Default Tagger.\n",
    "* Unigram Tagger.\n",
    "* Bigram Tagger.\n",
    "* Uma combinação entre eles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f246615-6872-4ff2-8ea6-56f93be5a067",
   "metadata": {},
   "source": [
    "### Default Tagger\n",
    "\n",
    "O Default Tagger atribui a mesma tag para cada token. Apesar de parecer simples, essa técnica estabelece um importante baseline para o desempenho do tagueador. Para isso, é preciso descobrir a tag mais frequente num Corpus e, com essa informação, criar um tagueador que atribuirá a todos os tokens essa tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70054e4e-6033-4789-aa17-9e081ccaec07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in twords]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33eebadd-b7ce-4a9a-9e42-e89953abdadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Esse', 'n'),\n",
       " ('é', 'n'),\n",
       " ('um', 'n'),\n",
       " ('exemplo', 'n'),\n",
       " ('utilizando', 'n'),\n",
       " ('o', 'n'),\n",
       " ('marcador', 'n'),\n",
       " ('padrão', 'n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'Esse é um exemplo utilizando o marcador padrão'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('n')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45c05ca2-3323-469d-a255-88c6cffc2401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('o', 'art'),\n",
       "  ('7_e_meio', 'prop'),\n",
       "  ('é', 'v-fin'),\n",
       "  ('um', 'art'),\n",
       "  ('ex-libris', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('a', 'art'),\n",
       "  ('noite', 'n'),\n",
       "  ('algarvia', 'adj'),\n",
       "  ('.', '.')],\n",
       " [('é', 'v-fin'),\n",
       "  ('uma', 'num'),\n",
       "  ('de', 'prp'),\n",
       "  ('as', 'art'),\n",
       "  ('mais', 'adv'),\n",
       "  ('antigas', 'adj'),\n",
       "  ('discotecas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('o', 'art'),\n",
       "  ('algarve', 'prop'),\n",
       "  (',', ','),\n",
       "  ('situada', 'v-pcp'),\n",
       "  ('em', 'prp'),\n",
       "  ('albufeira', 'prop'),\n",
       "  (',', ','),\n",
       "  ('que', 'pron-indp'),\n",
       "  ('continua', 'v-fin'),\n",
       "  ('a', 'prp'),\n",
       "  ('manter', 'v-inf'),\n",
       "  ('os', 'art'),\n",
       "  ('traços', 'n'),\n",
       "  ('decorativos', 'adj'),\n",
       "  ('e', 'conj-c'),\n",
       "  ('as', 'art'),\n",
       "  ('clientelas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('sempre', 'adv'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
    "train = tsents[1000:]\n",
    "test = tsents[:1000]\n",
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e7be09d-a132-4509-ab0d-bf9e2d9460f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17800040072129833\n"
     ]
    }
   ],
   "source": [
    "tagger0 = nltk.DefaultTagger('n')\n",
    "print(tagger0.accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af158b9a-f7f2-4df3-a19c-40eb297ed28d",
   "metadata": {},
   "source": [
    "O desempenho do Default Tagger é muito aquém do esperado, já que atribui a mesma tag para todas as palavras. Entretanto, estatisticamente e por conta do Corpus que estamos usando, quando tagueamos milhares de palavras num texto, a maioria das novas serão, de fato, substantivos. Dessa maneira, utilizar o Default Tagger pode ajudar a melhorar a robustez de um sistema de processamento de linguagem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f74eff-faea-425b-89b7-d8c91c9f3ccd",
   "metadata": {},
   "source": [
    "### Unigram Tagger\n",
    "\n",
    "A segunda abordagem que temos é o Unigram Tagger, que se baseia em frequência estatística da classe gramatical mais vezes atribuída a uma palavra. \n",
    "\n",
    "Em outras palavras, o Unigram Tagger estabelece a tag mais provável, por olhar para uma palavra, encontrar suas diferentes funções sintáticas dentro do Corpus, pegar aquela cuja recorrência seja máxima e atribuir essa tag para ocorrências dessa palavra no conjunto de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6194bf71-d75f-4555-bda8-2212120a7a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522139851733119\n"
     ]
    }
   ],
   "source": [
    "tagger1 = nltk.UnigramTagger(train)\n",
    "print(tagger1.accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1f015-b2c7-4040-9fd5-d42bb6874853",
   "metadata": {},
   "source": [
    "O conceito de Unigram Tagger pode ser generalizado para N-gram Tagger. O N-gram Tagger possui um contexto que é definido pelo token atual em conjunto com as tags dos n-1 tokens antecedentes. Observe a imagem abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdc375-b488-4e1c-bea5-978bc0ab948e",
   "metadata": {},
   "source": [
    "<img src=\"img/unigram_tagger.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9bdf5-7105-479b-9d25-ef0aac623e43",
   "metadata": {},
   "source": [
    "Aqui vamos usar o Bigram Tagger. entretanto, após treinar um Bigram, os resultados estão bem ruins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bbf362c-6e0a-4f5c-b564-8c74840ccd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14626327389300742\n"
     ]
    }
   ],
   "source": [
    "tagger2 = nltk.BigramTagger(train)\n",
    "print(tagger2.accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d6e31-e99d-461f-969e-eae0a95ea3e9",
   "metadata": {},
   "source": [
    "Por qual motivo isso aconteceu? Vamos entender:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180ae6e-b1f3-4f71-bdfa-cb9530bd25a6",
   "metadata": {},
   "source": [
    "<img src=\"img/bigram_tagger.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37610f-14df-445e-981e-c8cda9857503",
   "metadata": {},
   "source": [
    "Mesmo que a “tag n” seja a mesma no conjunto de treino e teste, ela não será rotulada corretamente, já que a tag n-1 é diferente. Consequentemente, o tagger falha em taguear o resto da sentença.\n",
    "\n",
    "À medida que “n” aumenta, a especificidade dos contextos aumenta, assim como a chance de que os dados que desejamos marcar contenham contextos que não estavam presentes nos dados de treinamento.\n",
    "\n",
    "Isto é conhecido como problema de esparsidade dos dados e é bastante recorrente em NLP. \n",
    "\n",
    "Como consequência, existe um trade-off entre acurácia e a cobertura dos resultados (relacionado com o trade-off de precision/recall em recuperação de informação).\n",
    "\n",
    "Uma maneira de resolver o trade-off entre acurácia e cobertura é utilizar o algoritmo com melhor acurácia que temos, mas retornar a algoritmos com maior cobertura quando necessário. \n",
    "\n",
    "Por exemplo, podemos combinar o resultado de um Bigram Tagger, Unigram Tagger e Default Tagger da seguinte maneira: \n",
    "\n",
    "* Tente taguear o token com o Bigram Tagger.\n",
    "* Se ele falhar, tente usar Unigram Tagger.\n",
    "* Se ele também falhar, use o Default Tagger.\n",
    "\n",
    "Para isso, usamos o conceito de backoff quando declaramos um Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60000788-9578-4b04-92a4-6539eaefcbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger1:  0.8740532959326788\n",
      "tagger2:  0.8900420757363254\n"
     ]
    }
   ],
   "source": [
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "print('tagger1: ',tagger1.accuracy(test))\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "print('tagger2: ',tagger2.accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093c112-3ecc-41d5-8c6f-53f9693b181f",
   "metadata": {},
   "source": [
    "Por fim, treinar um tagger pode consumir um tempo considerável num Corpus muito grande. Ao invés de treinar um tagger toda vez que precisarmos de um, é conveniente salvar um tagger treinado para posterior reuso. Assim, é possível carregar o modelo treinado e usá-lo em novos dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e8d0537-db3c-41a7-95a8-d1357c0b9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "output = open('tagger.pkl', 'wb')\n",
    "dump(tagger2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d057a9c-ff85-4b9e-abf1-ae14851d63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "input = open('tagger.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a5d99be-1cdf-4072-9b3f-90b8a5025d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1:  [('Isso', 'n'), ('é', 'v-fin'), ('para', 'prp'), ('você.', 'n')]\n",
      "text2:  [('para', 'prp'), ('com', 'prp'), ('isso', 'pron-indp')]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Isso é para você.\"\n",
    "text2 = \"para com isso\"\n",
    "tokens1 = text1.split()\n",
    "tokens2 = text2.split()\n",
    "print('text1: ',tagger.tag(tokens1))\n",
    "print('text2: ',tagger.tag(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aea2f6-93dd-4cfc-9b38-dd246ddeea37",
   "metadata": {},
   "source": [
    "# O que você viu nesta aula?\n",
    "\n",
    "Nesta aula, apresentamos o NLP, os desafios que temos, quais aplicações usam essa técnica e como machine learning pode nos auxiliar. Depois, entramos em detalhes para entender como realizar o tratamento de dados textuais, preparando-os para, futuramente, serem introduzidos no nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb458a-0a4e-411a-ac6e-831de6912ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
